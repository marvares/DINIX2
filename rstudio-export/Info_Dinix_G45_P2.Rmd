---
title: "Informe Estandarización Perú Escala DINI, Parte 2: Análisis de Ítems"
subtitle: "Muestra Nivel 4-5"
author: "Martín Vargas Estrada"
date: "`r Sys.time()`"
output:
  pdf_document:
    toc: true
    toc_depth: 4
  word_document:
    toc: true
    toc_depth: '4'
  html_document:
    toc: true
    toc_depth: '4'
    df_print: paged
header-includes: \renewcommand{\contentsname}{Índice} \renewcommand{\tablename}{Tabla}
---
\newpage

# Introducción


Informe de Exploración Psicométrica de los ìtems de la prueba DINI obtenidas con muestra de Perú, Niveles 4-5. 

# Profundización Análisis Factorial Confirmatorio (AFC)

Como se detalla en secciones previas, el FC del modelo resultó con bajo ajuste en la mayoría de índices; por lo tanto de pasará a profundizar en el análisis a fin de obtener el detalle de los ítems cuya modificación (o incluso supresión) podría generar una mejora en el ajuste del modelo.

```{r 45_P2_AFC_Indepth, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE, results='asis'}

library(tidyverse)
library(haven)
rm(list = ls()) 
# Carga el archivo .sav
INDI45 <- read_sav("INDI45.sav")

# Generar el DF solo con los valores de las subescalas

ayudin <- function(dataframe, columnas) {
  # Verificar que las columnas seleccionadas sean numéricas
  if (!all(sapply(columnas, function(col) is.numeric(dataframe[[col]])))) {
    stop("Todas las columnas seleccionadas deben ser numéricas.")
  }
  
  # Filtrar el dataframe eliminando filas con NA en las columnas seleccionadas
  dataframe_limpio <- dataframe[complete.cases(dataframe[, columnas]), columnas]
  
  # Devolver el dataframe limpio
  return(dataframe_limpio)
}



# Convertir columnas de la 3 a la 6 en un dataframe a numéricas
INDI45[ , 43:104] <- lapply(INDI45[ , 43:104], as.numeric)

# Llamar a la función y mostrar los resultados
LIMPION <- ayudin(dataframe = INDI45, columnas = c(43:105))

# Definir función que permitirá ejecutar AFC vía lavaan

library(lavaan)
library(knitr)

AFC <- function(data, factors, indicators, result_object_name, scale_name) {
  # Verificar que la longitud de `factors` coincida con la longitud de `indicators`
  if (length(factors) != length(indicators)) {
    stop("El número de factores debe coincidir con el número de listas de indicadores.")
  }

  # Generar la sintaxis del modelo automáticamente
  modelo <- ""
  for (i in seq_along(factors)) {
    factor_name <- factors[i]
    column_indices <- indicators[[i]]

    # Convertir los índices a nombres de las columnas
    variable_names <- colnames(data)[column_indices]
    variables <- paste(variable_names, collapse = " + ")
    modelo <- paste0(modelo, factor_name, " =~ ", variables, "\n")
  }

  # Ajustar el modelo usando lavaan::cfa()
  ajuste <- cfa(modelo, data = data)

  # Obtener índices de ajuste relevantes
  fit_indices <- fitmeasures(ajuste, c("rmsea", "cfi", "tli", "srmr", "gfi"))

  # Función auxiliar para evaluar el nivel obtenido
  evaluar_indice <- function(indice, valor) {
    case_when(
      indice == "rmsea" & valor <= 0.05 ~ "Excelente",
      indice == "rmsea" & valor <= 0.08 ~ "Aceptable",
      indice == "cfi" & valor >= 0.95 ~ "Excelente",
      indice == "cfi" & valor >= 0.90 ~ "Aceptable",
      indice == "tli" & valor >= 0.95 ~ "Excelente",
      indice == "tli" & valor >= 0.90 ~ "Aceptable",
      indice == "srmr" & valor <= 0.08 ~ "Excelente",
      indice == "srmr" & valor <= 0.10 ~ "Aceptable",
      indice == "gfi" & valor >= 0.95 ~ "Excelente",
      indice == "gfi" & valor >= 0.90 ~ "Aceptable",
      TRUE ~ "Deficiente"
    )
  }

  # Crear tabla con resultados (sin la columna con nombres de índices)
  resultados_tabla <- data.frame(
    "Índice Obtenido" = c(fit_indices["rmsea"], 
                          fit_indices["cfi"], 
                          fit_indices["tli"], 
                          fit_indices["srmr"], 
                          fit_indices["gfi"]),
    "Nivel Obtenido" = c(
      evaluar_indice("rmsea", fit_indices["rmsea"]),
      evaluar_indice("cfi", fit_indices["cfi"]),
      evaluar_indice("tli", fit_indices["tli"]),
      evaluar_indice("srmr", fit_indices["srmr"]),
      evaluar_indice("gfi", fit_indices["gfi"])
    )
  )

  # Cambiar nombres de las filas para usar como identificadores
  rownames(resultados_tabla) <- c("RMSEA", "CFI", "TLI/NNFI", "SRMR", "GFI")

  # Guardar el objeto con los resultados en el entorno global
  assign(result_object_name, ajuste, envir = .GlobalEnv)

  # Mostrar título y tabla formateada
  cat("\n### Resumen de Índices de Bondad de Ajuste para el AFC de la", scale_name, "\n\n")
  print(kable(resultados_tabla, align = "c", 
              caption = paste("Resumen de Índices de Bondad de Ajuste para el AFC de la", scale_name)))
}





# Ejecuto función

# Definición de argumentos


factores <- c("Cognitivo", "Motor", "Socioemocional", "Disposicional")
items <- list(c(1:26), c(27:32), c(33:46), c(47:52))
result_object_name <- "afc30_01"
escala <- "Escala DINI"


# Llamar a la función
resultado <- AFC (
  data = LIMPION, 
  factors = factores, 
  indicators = items, 
  result_object_name = "afc30_01",
  scale_name = escala
  )
# 
# summary(afc30_01, standardized = TRUE)

# Paquete necesario
library(knitr)

# Extraer las cargas factoriales estandarizadas del modelo
cargas <- inspect(afc30_01, what = "std")$lambda

# Convertir la matriz a un data.frame
cargas_df <- as.data.frame(cargas)
cargas_df <- cbind(Item = rownames(cargas_df), cargas_df)

# Identificar columnas con al menos una carga distinta de NA o 0
col_validas <- colSums(!is.na(cargas)) > 0  # Identifica factores con cargas
col_validas <- which(col_validas)           # Obtiene los índices de las columnas válidas

# Filtrar solo las columnas relevantes (ítems y factores con cargas válidas)
cargas_df <- cargas_df[, c(1, col_validas + 1)]  # "+1" porque "Item" está en la columna 1

# Renombrar columnas para mayor claridad
colnames(cargas_df)[-1] <- paste0("Factor", seq_len(ncol(cargas_df) - 1))

# Generar la tabla en formato Markdown con kable sin la columna extra
kable(cargas_df, caption = "Cargas Factoriales Estandarizadas", digits = 3, row.names = FALSE)



```

Para entender lo que las tablas anteriores nos indican es básico tomar en consideración lo siguiente:

1. Aplicando los criterios generalmente aceptados para la interpretación de AFC, consideraremos que todas las cargas factoriales estandarizadas inferiores a 0.4 en valor absoluto indicarían "red flags", es decir ítems que podrían considerarse para su eliminación o modificación. 
1. Es importante tomar en cuenta que los resultados estadísticos son solo parte del proceso de decisión en relación a la gestión de los ítems. La otra parte es el contenido de los ítems. Un ítem puede tener carga factorial negativa pero el signo podría estar totalmente justificado si, al analizar el contenido, comprobamos que en efecto la idea original al redactar el ítem era precisamente evaluar aspectos opuestos al factor o variable latente que se pretende medir. Por ejemplo una escala para medir depresión podría contener un ítem como "A menudo me siento entusiasta al pensar en mi futuro"; sería perfectamente lógico y coherente esperar que tal ítem tuviera una carga factorial negativa, al tratarse de un ítem inverso.
1. La meta al redactar un ítem es describir la variabilidad en la variable a medir. En ese sentido, estadísticamente hablando, la carga factorial no es más que la correlación entre el ítem y la variable o factor latente que se pretende medir. Una consecuencia de plantearse las cargas factoriales de esa forma es que es posible establecer qué proporción de la variabilidad de los datos es explicada por el factor o variable latente que queremos medir. En términos matemáticos, es posible cuantificar esa proporción elevando al cuadrado la carga factorial del caso. Por ejemplo, si un ítem tiene una carga factorial de 0.62, entonces matemáticamente un 38.44 % de su variabilidad (0.62 al cuadrado) estaría siendo explicada por la variable a medir. Es por esto que queremos excluir ítems con carga factorial menores a 0.4; por ejemplo, un ítem con carga factorial de 0.25 solo sería explicado en un 6.25% por la variable a medir. En otras palabras, el 93.75% de los datos de tal ítem se deberían a alguna otra variable (o combinación de variables), distinta a la variable que queremos medir. 

Ahora bien, sabedores de lo anterior, pasemos a señalar los conclusiones más saltantes:

1. Los ítems que componen la subescala "C" muestran cargas factoriales altas, lo cual indica que son estadísticamente válidos.
1. Los ítems de la subescala "M" también muestran cargas factoriales altas, lo cual indica que son estadísticamente válidos.
1. Los ítems de la subescala "S" tienen resultados variopintos. Para empezar, los ítems S1, S2, S7, S8 y S9 se comportan como ítems directos (a mayor puntaje en el ítem, mayor valor o presencia de la variable a medir), mientras que los ítems S3, S4, S5, S6, S10, S11, S12, S13 y S14 se comportan como ítems inversos. Todos los ítems directos muestran cargas factoriales aceptables, pero entre los ítems inversos S3, S4, S10, S11 parecen buenos candidatos para la supresión o, en todo caso, la modificación con vistas a su mejoramiento. 
1. Entre los ítems de la subescala "D", casi todos ostentan buenas cargas factoriales (siendo todos ítems directos). La excepción la constituye el ítem D3, el cual no solo es inverso (lo cual, como tenemos dicho, no es necesariamente un problema), sino que exhive una carga factorial muy baja de -.128; esto significa que apenas el 1.6% de su variabilidad es explicada por el factor que estamos mdiendo. 


Ya desde el punto de vista concpetual, podemos pasar a analizar el *contenido* de los ítems en cuestión. En especial, nos centraremos en los ítems "problemáticos". 

```{r 45_P2_Escalas_Descritas, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE, results='asis'}



library(readxl)
infovar <- read_excel("infovar.xlsx")

colnames(infovar) <- c("Contenido")

# Extraer las cargas factoriales estandarizadas del modelo
cargas <- inspect(afc30_01, what = "std")$lambda

# Convertir la matriz a un data.frame
cargas_df <- as.data.frame(cargas)
cargas_df <- cbind(Item = rownames(cargas_df), cargas_df)

# Identificar columnas con al menos una carga distinta de NA o 0
col_validas <- colSums(!is.na(cargas)) > 0  # Identifica factores con cargas
col_validas <- which(col_validas)           # Obtiene los índices de las columnas válidas

# Filtrar solo las columnas relevantes (ítems y factores con cargas válidas)
cargas_df <- cargas_df[, c(1, col_validas + 1)]  # "+1" porque "Item" está en la columna 1

# Renombrar columnas para mayor claridad
colnames(cargas_df)[-1] <- paste0("Factor", seq_len(ncol(cargas_df) - 1))

# Validar que el número de filas de infovar y cargas_df coincidan
if (nrow(cargas_df) != nrow(infovar)) {
  stop("Error: El número de filas en cargas_df no coincide con el número de filas en infovar.")
}

# Reemplazar la columna "Item" con los valores de "infovar$Contenido"
cargas_df$Item <- infovar$Contenido

# Generar la tabla en formato Markdown con kable
library(knitr)
kable(cargas_df, caption = "Cargas Factoriales Estandarizadas con Contenido", digits = 3, row.names = FALSE)

```

Podemos apreciar ahora que:

* Los ítems de la subescala "C" tienen validez aparente: su contenido parece relacionado con el factor a medir.
* Los ítems de la subescala "M" tienen validez aparente: su contenido parece relacionado con el factor a medir.
* Los ítems inversos de la subescala "S" parecen efectivamente medir la variable latente en sentido opuesto, por lo que es plenamente justificado que tengan cargas factoriales negativas. Dicho esto, la mayoría de ítems inversos muestran también cargas factoriales relativamente bajas. En especial, S3 ("Evita relacionarse con otros en diferentes situaciones"); S4 ("Muestra aprensión y/o excesiva preocupación ante sucesos futuros"); S10 ("Se muestra tímido y/o no comparte sus experiencias con el grupo"); y S11 ("Tiene un aspecto triste, cansado y/o preocupado"). El resto de ítems inversos, si bien se ubican en valores iguales o ligeramente por encima de 0.4 en valor absoluto, ciertamente  evidencian una carga factorial menor a las sus contrapartes directas. Las razones de esta relativa ineficiencia de los ítems inversos de la subescala "S" puede deberse o bien a aspectos de contenido (los ítems no son adecuadamente interpretados o quizás haya un efecto cultural/lingüístico), o a aspectos metodológicos (puede que haya existido un efecto debido a la secuencia de administración).
* Los ítems de la subescala "D" parecen reflejar apropiadamente la dimensión a evaluar. Aun así, el ítem D3 ("Necesita que lo motiven para la realización de las actividades") también evidencia una carga factorial bastante débil. Si se recuerda, también en los resultados de la confiabilidad encontramos que la confiabilidad aumentaría con su supresión. 


# Análisis de Ítems: Outliers y Transformaciones de Escala

## Análisis de Outliers

```{r 45_P2_Outliers, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE, results='asis'}

knitr::opts_chunk$set(
	echo = FALSE,
	message = FALSE,
	warning = FALSE
	)
knitr::opts_current$get('Tabla')


library(dplyr)
INDI45_CATEGO <- INDI45 %>% 
  dplyr::select(Codigo, Regnat, Reg)

# Extraigo los campos categóricos que me interesan del df original
# Transformo los valores correspondientes a factor
# Cambiar la variable "Regnat" a factor y reetiquetar sus valores
INDI45_CATEGO$Regnat <- factor(INDI45_CATEGO$Regnat, 
                            levels = c("1", "2", "3"),  # Valores originales
                            labels = c("Costa", "Sierra", "Selva"))  # Nuevas etiquetas
# Cambiar la variable "Regnat" a factor y reetiquetar sus valores
INDI45_CATEGO$Reg <- factor(INDI45_CATEGO$Reg)  # Nuevas etiquetas
  
PRELIMPIO <- ayudin(dataframe = INDI45, columnas = c(1, 44:105)) # Extraigo solo valores no NA y numéricos del df original
# Realizar el left join
LIMPIO <- left_join(PRELIMPIO, INDI45_CATEGO, by = "Codigo")


# Función para identificar casos con valores atípicos (outliers) en columnas específicas
items_intensos <- function(data, rango_columnas, columna_contexto, multiplicador_IQR = 1.5) {
  # Verificar que las columnas estén en el rango
  columnas_seleccionadas <- names(data)[rango_columnas]
  
  # Identificar columnas no numéricas
  columnas_no_numericas <- columnas_seleccionadas[!sapply(data[, rango_columnas, drop = FALSE], is.numeric)]
  
  # Excluir las columnas no numéricas y avisar
  columnas_a_evaluar <- setdiff(columnas_seleccionadas, columnas_no_numericas)
  if (length(columnas_no_numericas) > 0) {
    cat("Las siguientes columnas no son numéricas y serán excluidas del análisis:\n")
    print(columnas_no_numericas)
  }
  
  # Inicializar una tabla consolidada
  resultados_outliers <- data.frame(
    Ítem = character(),
    Caso = integer(),
    Valor_Outlier = numeric(),
    Región = character()
  )
  
  # Aplicar la detección de outliers para cada columna numérica
  for (columna in columnas_a_evaluar) {
    # Obtener los datos de la columna
    valores <- data[[columna]]
    
    # Calcular los cuartiles y los límites del IQR
    Q1 <- quantile(valores, 0.25, na.rm = TRUE)
    Q3 <- quantile(valores, 0.75, na.rm = TRUE)
    IQR <- Q3 - Q1
    limite_inferior <- Q1 - multiplicador_IQR * IQR
    limite_superior <- Q3 + multiplicador_IQR * IQR
    
    # Identificar las filas con outliers
    filas_outliers <- which(valores < limite_inferior | valores > limite_superior)
    
    # Crear un data frame con los resultados para esta columna
    if (length(filas_outliers) > 0) {
      resultados <- data.frame(
        Columna = columna,
        Fila = filas_outliers,
        Valor_Outlier = valores[filas_outliers],
        Contexto = data[[columna_contexto]][filas_outliers]
      )
      
      # Consolidar los resultados
      resultados_outliers <- rbind(resultados_outliers, resultados)
    }
  }
  
  # Generar una tabla en Markdown
  if (nrow(resultados_outliers) > 0) {
    cat("Tabla consolidada de casos con valores atípicos\n")
    kable(resultados_outliers, format = "markdown", row.names = FALSE)
  } else {
    cat("No se detectaron valores atípicos en las columnas evaluadas.\n")
  }
}


# EJEMPLO
# Ejecutar la función para encontrar casos con valores atípicos
items_intensos(
  data = LIMPIO,           # Data frame con los datos
  rango_columnas = 2:53,     # Rango de columnas numéricas a evaluar (columnas 2 a 4)
  columna_contexto = "Regnat",  # Columna adicional para incluir en la salida (contexto)
  multiplicador_IQR = 3   # Multiplicador del IQR (por defecto 1.5)
)


```
Para resumir, podemos ofrecer las tablas siguientes, que resumen qué items muestran valores atípicos y cuántos casos hay, así como la frecuencia de valores atípicos según las categorías de Región (Costa, Sierra o Selva), y Departamento:

```{r 45_Outliers_Resumen, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE, results='asis'}

# Función para resumir outliers
library(dplyr)
library(knitr)

items_intensos_resumen <- function(data, rango_columnas, columna_contexto, multiplicador_IQR = 1.5) {
  library(dplyr)
  library(knitr)
  
  # Verificar las columnas numéricas dentro del rango
  columnas_seleccionadas <- names(data)[rango_columnas]
  columnas_no_numericas <- columnas_seleccionadas[!sapply(data[, rango_columnas, drop = FALSE], is.numeric)]
  columnas_a_evaluar <- setdiff(columnas_seleccionadas, columnas_no_numericas)
  
  if (length(columnas_no_numericas) > 0) {
    cat("Las siguientes columnas no son numéricas y serán excluidas del análisis:\n")
    print(columnas_no_numericas)
  }
  
  # Crear un data frame vacío para registrar los outliers
  registros_outliers <- data.frame(
    Columna = character(),
    Valor_Outlier = numeric(),
    Contexto = character()
  )
  
  # Identificar los outliers en cada columna numérica
  for (columna in columnas_a_evaluar) {
    valores <- data[[columna]]
    Q1 <- quantile(valores, 0.25, na.rm = TRUE)
    Q3 <- quantile(valores, 0.75, na.rm = TRUE)
    IQR <- Q3 - Q1
    limite_inferior <- Q1 - multiplicador_IQR * IQR
    limite_superior <- Q3 + multiplicador_IQR * IQR
    filas_outliers <- which(valores < limite_inferior | valores > limite_superior)
    
    if (length(filas_outliers) > 0) {
      registros <- data.frame(
        Columna = columna,
        Valor_Outlier = valores[filas_outliers],
        Contexto = data[[columna_contexto]][filas_outliers]
      )
      registros_outliers <- bind_rows(registros_outliers, registros)
    }
  }
  
  if (nrow(registros_outliers) > 0) {
    # Matriz de doble entrada: sumar frecuencia por ítem y valor atípico
    matriz_doble_entrada <- registros_outliers %>%
      group_by(Columna, Valor_Outlier) %>%
      summarise(Frecuencia = n(), .groups = "drop") %>%
      arrange(desc(Frecuencia))  # Ordenar por frecuencia descendente
    
    # Tabla de frecuencias descendentes del contexto
    tabla_frecuencias <- registros_outliers %>%
      count(Contexto, sort = TRUE, name = "Frecuencia")
    
    # Imprimir tablas usando kable
    cat("\n### Resumen de Outliers por ítem\n")
    print(kable(matriz_doble_entrada, format = "markdown", row.names = FALSE))
    
    cat("\n### Tabla de Outliers según Categoría\n")
    print(kable(tabla_frecuencias, format = "markdown", row.names = FALSE))
  } else {
    cat("No se detectaron valores atípicos en las columnas evaluadas.\n")
  }
}


# Llamando a la función:

items_intensos_resumen(
  data = LIMPIO,           # Data frame con los datos
  rango_columnas = 2:53,     # Rango de columnas numéricas a evaluar (columnas 2 a 4)
  columna_contexto = "Regnat",  # Columna adicional para incluir en la salida (contexto)
  multiplicador_IQR = 3   # Multiplicador del IQR (por defecto 1.5)
)

# Como soy un capo, genero función solo para que se muestre la segunda tabla de categorías:

items_intensos_catego <- function(data, rango_columnas, columna_contexto, multiplicador_IQR = 1.5) {
  library(dplyr)
  library(knitr)
  
  # Verificar las columnas numéricas dentro del rango
  columnas_seleccionadas <- names(data)[rango_columnas]
  columnas_no_numericas <- columnas_seleccionadas[!sapply(data[, rango_columnas, drop = FALSE], is.numeric)]
  columnas_a_evaluar <- setdiff(columnas_seleccionadas, columnas_no_numericas)
  
  if (length(columnas_no_numericas) > 0) {
    cat("Las siguientes columnas no son numéricas y serán excluidas del análisis:\n")
    print(columnas_no_numericas)
  }
  
  # Crear un data frame vacío para registrar los outliers
  registros_outliers <- data.frame(
    Columna = character(),
    Valor_Outlier = numeric(),
    Contexto = character()
  )
  
  # Identificar los outliers en cada columna numérica
  for (columna in columnas_a_evaluar) {
    valores <- data[[columna]]
    Q1 <- quantile(valores, 0.25, na.rm = TRUE)
    Q3 <- quantile(valores, 0.75, na.rm = TRUE)
    IQR <- Q3 - Q1
    limite_inferior <- Q1 - multiplicador_IQR * IQR
    limite_superior <- Q3 + multiplicador_IQR * IQR
    filas_outliers <- which(valores < limite_inferior | valores > limite_superior)
    
    if (length(filas_outliers) > 0) {
      registros <- data.frame(
        Columna = columna,
        Valor_Outlier = valores[filas_outliers],
        Contexto = data[[columna_contexto]][filas_outliers]
      )
      registros_outliers <- bind_rows(registros_outliers, registros)
    }
  }
  
  if (nrow(registros_outliers) > 0) {
    # Tabla de frecuencias descendentes del contexto
    tabla_frecuencias <- registros_outliers %>%
      count(Contexto, sort = TRUE, name = "Frecuencia")
    
    # Imprimir tabla usando kable
    cat("\n### Tabla de Outliers según Categoría\n")
    print(kable(tabla_frecuencias, format = "markdown", row.names = FALSE))
  } else {
    cat("No se detectaron valores atípicos en las columnas evaluadas.\n")
  }
}

# Ahora la aplico, muy frescamente

items_intensos_catego (
  data = LIMPIO,           # Data frame con los datos
  rango_columnas = 2:53,     # Rango de columnas numéricas a evaluar (columnas 2 a 4)
  columna_contexto = "Reg",  # Columna adicional para incluir en la salida (contexto)
  multiplicador_IQR = 3   # Multiplicador del IQR (por defecto 1.5)
)

```

Las conclusiones apuntan a que la única escala que contiene valores extremos en la escala S, en especial los ítems S6, S12 y S14. Hay que señalar que los parámetros elegidos para definir "valor extremo" (tres veces el rango intercuartílico, en lugar del nivel usual de rango y medio) son extremos en verdad; la distribución de los puntajes es bastante dispersa (incluir aquí la DS promedio para todos los ítemes).

Una vez más, esto apunta en una dirección bastante clara: los subgrupos son tan disitntos entre sí que incluirlos todos dentro de la misma "muestra general" hace que la variabilidad se eleve bastante y la aparición de outliers se haga más probable.

# Resumen de Ítems Binarios (NQV, pero...)

```{r 45_P2_Resumen_Binarios, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE, results='asis'}

# Cargar librerías necesarias
library(dplyr)
library(ggplot2)
library(tidyr)
library(kableExtra)
library(RColorBrewer)

# Función ajustada
anal_di <- function(data, 
                    rango_indices, 
                    titulo_tabla = "Tabla de porcentajes de 'Sí' (1)", 
                    titulo_grafico = "Porcentajes de 'Sí' (1) por variable", 
                    color_contorno = "black", 
                    paleta_colores = "Dark2") {
  # Seleccionar las variables según el rango
  sub_data <- data[, rango_indices]
  
  # Verificar que las columnas sean dicotómicas
  if (!all(apply(sub_data, 2, function(x) all(x %in% c(0, 1))))) {
    stop("Todas las columnas seleccionadas deben ser dicotómicas (valores 0 y 1).")
  }
  
  # Calcular los porcentajes de "1" para cada variable
  tabla_porcentajes <- sub_data %>%
    summarise(across(everything(), ~ mean(.x) * 100)) %>%
    pivot_longer(cols = everything(), names_to = "Variable", values_to = "Porcentaje") %>%
    arrange(desc(Porcentaje)) %>%
    mutate(Porcentaje = round(Porcentaje, 2)) # Redondear a 2 decimales
  
  # Mostrar la tabla en formato Markdown usando kable
  tabla_md <- tabla_porcentajes %>%
    kable("markdown", col.names = c("Variable", "Porcentaje (%)"), caption = titulo_tabla) 
  # %>%
  #   kable_styling(full_width = FALSE, bootstrap_options = c("striped", "hover"))
  
  print(tabla_md)
  
  # Crear el gráfico de barras
  grafico <- ggplot(tabla_porcentajes, aes(x = reorder(Variable, -Porcentaje), y = Porcentaje, fill = Variable)) +
    geom_col(width = 0.75, color = color_contorno) +  # Agregar línea de contorno
    geom_text(aes(label = paste0(round(Porcentaje, 1), "%")), vjust = -0.5) +
    scale_fill_brewer(palette = paleta_colores) +  # Paleta definida por el usuario
    labs(
      title = titulo_grafico,
      x = "Variables",
      y = "Porcentaje",
      caption = "Fuente: Data Niveles 4-5"
    ) +
    theme_gray() +  # Cambiar tema a theme_gray
    theme(legend.position = "none")
  
  print(grafico)
}


# Ejemplo de uso



anal_di(data = INDI45, 
        rango_indices = c(22:30), 
        titulo_tabla = "Resumen de Incidencias", 
        titulo_grafico = "Porcentajes de Incidencia", 
        color_contorno = "black", 
        paleta_colores = "Greens")


anal_di(data = INDI45, 
        rango_indices = c(33:39), 
        titulo_tabla = "Resumen de Tratamientos", 
        titulo_grafico = "Porcentajes de Tratamiento", 
        color_contorno = "black", 
        paleta_colores = "Blues")

# resumen_binario(INDI45, columnas = c(22:30), equivalencias = c("1","0"),
#                             titulo_grafico = "Distribución de Respuestas 'Sí'",
#                             etiqueta_x = "Respuestas Sí",
#                             etiqueta_y = "Porcentaje",
#                             pie_de_pagina = "Fuente: Nivel 4-5",
#                             paleta_colores = "Oranges",
#                             color_borde = "black")
# 
# resumen_binario(INDI45, columnas = c(33:39), equivalencias = c("1","0"),
#                             titulo_grafico = "Distribución de Respuestas 'Sí'",
#                             etiqueta_x = "Respuestas Sí",
#                             etiqueta_y = "Porcentaje",
#                             pie_de_pagina = "Fuente: Nivel 4-5",
#                             paleta_colores = "Oranges",
#                             color_borde = "black")

```

En cuanto a las incidencias, las más frecuentes en la muestra son las clasificadas como "Otros", seguidas de las incidencias de Desempleo, de Mudanza y de Tratamiento Médico.

En relación a los tratamientos, la más frecuente es la que corresponde a tratamiento psicológico, seguida de....



# Transformaciones de las Escalas

Pasaremos a ejecutar dos transformaciones de la data. Las transformaciones son algoritmos que aplican modificaciones sistemáticas a  cada uno de los datos a fin de obtener data transformada cuya distribución, potencialmente, podría acercarse a una distribución normal. 

Ejecutaremos dos transformaciones: logarítmica y Box-Cox. Se eligió esas dos porque son las que usualmente generan los mejores resultados.

## Transformación Logarítmica

```{r 45_P2_Transformaciones_Log, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE, results='asis'}
library(nortest) # Para el test de Lilliefors
library(car)     # Para la transformación Box-Cox
library(knitr)   # Para salida tabular bonitamente
library(dplyr)

# Función para calcular y evaluar normalidad con transformaciones
library(nortest) # Para el test de Lilliefors (Kolmogorov-Smirnov corregido)
library(knitr)   # Para la salida tabular

# Función para transformar logarítmicamente y evaluar normalidad
log_transform_norma <- function(data) {
  # Verificar que las columnas sean numéricas
  data <- data[, sapply(data, is.numeric)]
  
  # Aplicar la transformación logarítmica (con manejo de valores <= 0)
  log_transformada <- as.data.frame(lapply(data, function(x) {
    if (any(x <= 0)) return(rep(NA, length(x))) else return(log(x))
  }))
  
  # Evaluar normalidad con Shapiro-Wilk y Lilliefors
  evaluar_normalidad <- function(variable) {
    if (length(variable[!is.na(variable)]) > 2) {
      shapiro_p <- tryCatch(shapiro.test(variable)$p.value, error = function(e) NA)
      ks_p <- tryCatch(nortest::lillie.test(variable)$p.value, error = function(e) NA)
    } else {
      shapiro_p <- ks_p <- NA
    }
    list(Shapiro_Wilk = shapiro_p, Lilliefors_KS = ks_p)
  }
  
  # Crear un dataframe para almacenar los resultados
  resultados <- data.frame(
    Escala = character(),
    Índice = character(),
    `p-valor` = character(),
    Interpretación = character(),
    stringsAsFactors = FALSE
  )
  
  interpretar_resultado <- function(p) {
    if (is.na(p)) return(c("NA", "Valor insuficiente"))
    nivel_significancia <- ifelse(p <= 0.001, "***",
                                  ifelse(p <= 0.01, "**",
                                         ifelse(p <= 0.05, "*", "NS")))
    interpretacion <- ifelse(p > 0.05, "Normalidad", "No Normalidad")
    c(sprintf("%.2f, %s", p, nivel_significancia), interpretacion)
  }
  
  # Iterar sobre cada columna
  for (var in colnames(log_transformada)) {
    variable <- log_transformada[[var]]
    resultados_test <- evaluar_normalidad(variable)
    
    # Agregar resultados de Shapiro-Wilk
    sw_resultado <- interpretar_resultado(resultados_test$Shapiro_Wilk)
    resultados <- rbind(resultados, data.frame(
      Escala = var,
      Índice = "S-W",
      `p-valor` = sw_resultado[1],
      Interpretación = sw_resultado[2],
      stringsAsFactors = FALSE
    ))
    
    # Agregar resultados de Lilliefors
    ks_resultado <- interpretar_resultado(resultados_test$Lilliefors_KS)
    resultados <- rbind(resultados, data.frame(
      Escala = var,
      Índice = "K-S, L",
      `p-valor` = ks_resultado[1],
      Interpretación = ks_resultado[2],
      stringsAsFactors = FALSE
    ))
  }
  
  # Mostrar resultados como tabla
  kable(resultados, format = "markdown", caption = "Transformación Logarítmica de las Escalas")
}

# Preparo data

# PULCRO <- LIMPIO %>% 
#   select(CSUM:DSUM)
PULCRO <- LIMPIO[, c(60:62)]
# Ejecutar la función
log_transform_norma(PULCRO)

```


Los resultados indican que, aun después de la transformación logarítmica, la distribución de los datos sigue sin satisfacer los criterios de normalidad.


Pasemos ahora a verificar los resultados de la transformación Box-Cox.


## Transformación Box-Cox

A continuación, mostramos los resultados de la transformación Box-Cox. 


```{r 45_P2_Transformaciones_BoxCox, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE, results='asis'}
# Función para aplicar la transformación Box-Cox y evaluar normalidad
# Cargar librerías necesarias
library(MASS)       # Para la transformación Box-Cox
library(nortest)    # Para el test de Lilliefors (K-S corregido)
library(knitr)      # Para mostrar tablas en formato markdown

# Función para aplicar la transformación Box-Cox y evaluar normalidad
box_norma <- function(data) {
  # Cargar la librería MASS si no está ya cargada
  if (!requireNamespace("MASS", quietly = TRUE)) {
    install.packages("MASS")
  }
  library(MASS)
  
  # Verificar que las columnas sean numéricas
  data <- data[, sapply(data, is.numeric)]
  
  # Aplicar la transformación Box-Cox
  box_transformada <- as.data.frame(lapply(data, function(x) {
    if (any(x <= 0)) return(rep(NA, length(x))) 
    else {
      bc <- MASS::boxcox(x ~ 1, plotit = FALSE)
      lambda <- bc$x[which.max(bc$y)]
      if (lambda == 0) return(log(x))
      else return((x^lambda - 1) / lambda)
    }
  }))
  
  # Evaluar normalidad con Shapiro-Wilk y Lilliefors
  evaluar_normalidad <- function(variable) {
    if (length(variable[!is.na(variable)]) > 2) {
      shapiro_p <- tryCatch(shapiro.test(variable)$p.value, error = function(e) NA)
      ks_p <- tryCatch(nortest::lillie.test(variable)$p.value, error = function(e) NA)
    } else {
      shapiro_p <- ks_p <- NA
    }
    list(Shapiro_Wilk = shapiro_p, Lilliefors_KS = ks_p)
  }
  
  # Crear un dataframe para almacenar los resultados
  resultados <- data.frame(
    Escala = character(),
    Índice = character(),
    `p-valor` = character(),
    Interpretación = character(),
    stringsAsFactors = FALSE
  )
  
  interpretar_resultado <- function(p) {
    if (is.na(p)) return(c("NA", "Valor insuficiente"))
    nivel_significancia <- ifelse(p <= 0.001, "***",
                                  ifelse(p <= 0.01, "**",
                                         ifelse(p <= 0.05, "*", "NS")))
    interpretacion <- ifelse(p > 0.05, "Normalidad", "No Normalidad")
    c(sprintf("%.2f, %s", p, nivel_significancia), interpretacion)
  }
  
  # Iterar sobre cada columna
  for (var in colnames(box_transformada)) {
    variable <- box_transformada[[var]]
    resultados_test <- evaluar_normalidad(variable)
    
    # Agregar resultados de Shapiro-Wilk
    sw_resultado <- interpretar_resultado(resultados_test$Shapiro_Wilk)
    resultados <- rbind(resultados, data.frame(
      Escala = var,
      Índice = "S-W",
      `p-valor` = sw_resultado[1],
      Interpretación = sw_resultado[2],
      stringsAsFactors = FALSE
    ))
    
    # Agregar resultados de Lilliefors
    ks_resultado <- interpretar_resultado(resultados_test$Lilliefors_KS)
    resultados <- rbind(resultados, data.frame(
      Escala = var,
      Índice = "K-S, L",
      `p-valor` = ks_resultado[1],
      Interpretación = ks_resultado[2],
      stringsAsFactors = FALSE
    ))
  }
  
  # Mostrar resultados como tabla
  kable(resultados, format = "markdown", caption = "Transformación Box-Cox de las Escalas")
}


PULCRO <- LIMPIO[, c(60:62)]
# Ejecutar la función
# Aplicar la transformación Box-Cox
box_norma(PULCRO)

```

Los resultados de la transformación Box-Cox tampoco logran que los datos transformados se acerquen significativamente a una distribución normal. 


### Conclusiones

* En general, se verifica que estas dos transformaciones sistemáticas de la data no logran acercarla significativamente a la normalidad. 
* Si bien es posible seguir intentando con otros algoritmos (raíz cuadrada, inversa, etc.), es nuestra recomendación pasar al análisis ulterior, ya que en estos casos el tamaño muestral justifica matemáticamente el uso de herramientas paramétricas. 
* Más allá de lo metodológico, y viendo la distribución gráfica los datos (Ver Parte 1), se recomienda verificar las razones por las que una muestra tan grande evidencia una distribución no normal. 
* El presente análisis se planteará en lo subsiguiente enfocarse en esta cuestión, la cual se convierte en relevante desde el punto de vista conceptual. 


# Análisis de Ítems

En esta sección pasaremos a analizar la calidad de los ítems de la escala INDI, en su aplicación a la muestra peruana que nos ocupa.

Emoezaremos por recalcular los puntajes sumatorios en vista de que existen ítems inversos.

```{r 45_P2_Indice_Discri, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE, results='asis'}
# LIMPIO es nuestro df de punto de partida para este propósito.  

# Recreamos las variables sumatorias:

LIMPIO$B_CSUM <- rowSums(LIMPIO[, 2:26])
LIMPIO$B_MSUM <- rowSums(LIMPIO[, 27:32])

# Crear nuevas columnas para los ítems invertidos
# unique(LIMPIO$S3)
LIMPIO$D3_i <- 6 + 1 - LIMPIO$D3

# Mover la columna `item_final` a la posición 3
# Items inversos en S: S3, S4, S5, S6, S10, S11, S12, S13, S14
LIMPIO$S3_i <- 6 + 1 - LIMPIO$S3
LIMPIO$S4_i <- 6 + 1 - LIMPIO$S4
LIMPIO$S5_i <- 6 + 1 - LIMPIO$S5
LIMPIO$S6_i <- 6 + 1 - LIMPIO$S6
LIMPIO$S10_i <- 6 + 1 - LIMPIO$S10
LIMPIO$S11_i <- 6 + 1 - LIMPIO$S11
LIMPIO$S12_i <- 6 + 1 - LIMPIO$S12
LIMPIO$S13_i <- 6 + 1 - LIMPIO$S13
LIMPIO$S14_i <- 6 + 1 - LIMPIO$S14

LIMPIO <- LIMPIO %>%
  relocate(D3_i, .before = D4)
LIMPIO$B_DSUM <- rowSums(LIMPIO[, c(47:48, 50:53)])
# LIMPIO <- LIMPIO %>%
#   relocate(S3_i, .before = S4)
# LIMPIO <- LIMPIO %>%
#   relocate(S4_i, .before = S5)
LIMPIO$SSUM <- rowSums(LIMPIO[, c(33:46)])
LIMPIO$B_SSUM <- rowSums(LIMPIO[, c(33:34, 39:41, 69:77)])


# Comparar si las dos columnas son idénticas
identical(LIMPIO$CSUM, LIMPIO$B_CSUM)
identical(LIMPIO$MSUM, LIMPIO$B_MSUM)
identical(LIMPIO$DSUM, LIMPIO$B_DSUM)
identical(LIMPIO$DSUM, LIMPIO$B_SSUM)
```

