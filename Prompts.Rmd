---
title: "Prompts"
author: "Martín Vargas Estrada"
date: "`r Sys.Date()`"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

En el contexto del lenguaje de programación estadística R, 
## Spearman

Necesito que generes una función que evalúe la correlación Spearman entre una serie de variables numéricas, por un lado, y una única variable numérica, por el otro. Llamemos "X" a las variables numéricas múltiples, y "Y" a la única variable contra la cual se va a evaluar la asociación o correlación. Por ejemplo, las variables "X" podrían ser Puntaje en el curso de Matemática(M), Puntaje en el curso de Lenguaje(L), y Puntaje en el curso de Idiomas(I). La variable "Y" podría ser: Coeficiente Intelectual(CI). Entonces las correlaciones a calcular serían: M y CI; L y CI, I y CI. 

La idea es que los parámetros sean: 

1. Dataframe
2. Variables "X" (una o más)
3. Variable "Y" (solo una)

El resultado debería ser una tabla de doble entrada con los nombres de las variables "X" en las filas, y tres columnas. La primera debe llamarse "Índice", y contendrá el valor del índice rho; la segunda "Magnitud", y contendrá el valor según lo que indico más abajo; la tercera columna debe llamarse "Significación" y debe contener el nivel de significancia estadística (según lo usual: ***,  **, *, NS).

Para la magnitud, básate en lo siguiente (valores absolutos): 

1. Si 0 ≥ rho > 0.1 La magnitud es nula
2. Si 0.1 ≥ rho > 0.2 La magnitud es muy débil
3. Si 0.2 ≥ rho > 0.3 La magnitud es débil
4. Si 0.3 ≥ rho > 0.5 La magnitud es moderada
5. Si 0.5 ≥ rho > 0.7 La magnitud es fuerte
6. Si 0.7 ≥ rho > 1.0 La magnitud es muy fuerte

Para el sentido, básate en lo siguiente: 
a. Si rho > 0, la asociación es directa
b. Si rho < 0, la asociación es inversa

Luego de la tabla, deberás generar un primer párrafo que enumere las correlaciones que hayan sido estadísticamente significativas, así como las que no lo han sido. Ejemplo: "La siguiente correlación resultó no ser estadísticamente significativa: Puntaje en el curso de Matemática y CI. Por otro lado, las siguientes sí resultaron ser estadísticamente significativas: Puntaje en el curso de Lenguaje y CI; y Puntaje en el curso de Idiomas y CI. 

Luego, debes escribir sendos párrafos con la interpretación de cada índice rho que sea estadísticamente significativo. Asimismo, debes tomar en cuenta el signo del índice rho (positivo o negativo). Cada párrafo deberá enunciar la interpretación de cada una de las correlaciones a partir de la magnitud y el sentido. Por ejemplo, asumamos que la correlación entre Puntaje en el curso de Lenguaje y CI es 0.41, y la la correlación entre Puntaje en el curso de Idiomas y CI es -0.23. Entonces en este caso la interpretación sería: "A continuación pasaremos a analizar el detalle de las correlaciones estadísticamente significativas. La asociación entre Puntaje en el curso de Lenguaje y CI es moderada y directa; esto implica que a mayor puntaje en el curso de Lenguaje, la persona suele tener mayores puntajes en CI. Mientras que la asociación entre Puntaje en el curso de Idiomas  y CI es débil e inversa; esto implica que a mayor puntaje en el curso de Idiomas, la persona suele tener mayores pountajes de CI. "

Asume que el data frame solo contendrá valores numéricos sin NAs. 

Me puedes hacer todas las preguntas que consideres necesarias para dar tu mejor resultado.

## Respuestas

1. Puedes referirte a las variables usando el número de columnas, por ejemplo, las variables X podrían ser c(2:5), mientras que la variable Y podría ser c(6).
2. La tabla debe estar en formato Flextable compatible con Ms Word. Imprímerlo directamente, no hay necesidad de guardarlo como objeto.
3. Sí, puedes asumir los valores estándar de significancia (p < 0.001 = "", p < 0.01 = "", p < 0.05 = "", p ≥ 0.05 = "NS")
4. Sí, es necesario incluir ejemplos detallados para cada asociación estadísticamente significativa, tal como expliqué en el ejemplo.

### Spearman v2

En la función "corre_anal", definida según lo siguiente:

# Función para calcular y analizar las correlaciones de Spearman con formato Markdown ajustado
corre_anal <- function(data, variables_X, variable_Y) {
  resultados <- data.frame(Variable_X = character(), Rho = numeric(), Magnitud = character(), 
                           Significacion = character(), stringsAsFactors = FALSE)
  
  # Iterar sobre las variables X
  for (var in variables_X) {
    test <- cor.test(data[[var]], data[[variable_Y]], method = "spearman")
    rho <- test$estimate
    p_value <- test$p.value
    
    # Determinar la magnitud de rho
    magnitud <- ifelse(abs(rho) > 0.7, "Muy fuerte",
                       ifelse(abs(rho) > 0.5, "Fuerte",
                              ifelse(abs(rho) > 0.3, "Moderada",
                                     ifelse(abs(rho) > 0.2, "Débil",
                                            ifelse(abs(rho) > 0.1, "Muy débil", "Nula")))))
    
    # Determinar la significación estadística
    significacion <- ifelse(p_value < 0.001, "***",
                            ifelse(p_value < 0.01, "**",
                                   ifelse(p_value < 0.05, "*", "NS")))
    
    # Agregar los resultados a la tabla
    resultados <- rbind(resultados, data.frame(
      Variable_X = colnames(data)[var],
      Rho = round(rho, 2),  # Redondear a dos decimales
      Magnitud = magnitud,
      Significacion = significacion
    ))
  }
  
  # Crear la tabla en formato Markdown
  markdown_tabla <- paste0(
    "| Variable X        | Índice Rho | Magnitud     | Significación |\n",
    "|-------------------|------------|--------------|---------------|\n"
  )
  for (i in 1:nrow(resultados)) {
    fila <- resultados[i, ]
    # Alinear Rho a la derecha añadiendo espacios si es necesario
    rho_formateado <- sprintf("%6.2f", fila$Rho)
    markdown_tabla <- paste0(markdown_tabla,
                             "| ", fila$Variable_X, 
                             " | ", rho_formateado, 
                             " | ", fila$Magnitud, 
                             " | ", fila$Significacion, "         |\n")
  }
  
  # Generar interpretaciones textuales
  significativas <- resultados[resultados$Significacion != "NS", ]
  no_significativas <- resultados[resultados$Significacion == "NS", ]
  
  texto <- ""
  if (nrow(no_significativas) > 0) {
    texto <- paste(texto, "La(s) siguiente(s) correlación(es) no fueron estadísticamente significativas:")
    texto <- paste(texto, paste(no_significativas$Variable_X, collapse = ", "), ".")
  }
  
  if (nrow(significativas) > 0) {
    texto <- paste(texto, "Por otro lado, las siguientes correlaciones sí resultaron estadísticamente significativas:")
    texto <- paste(texto, paste(significativas$Variable_X, collapse = ", "), ".")
    
    texto <- paste(texto, "\n\nA continuación, pasaremos a analizar el detalle de las correlaciones estadísticamente significativas.\n")
    for (i in 1:nrow(significativas)) {
      detalle <- significativas[i, ]
      signo <- ifelse(detalle$Rho > 0, "directa", "inversa")
      interpretacion <- paste0(
        "La asociación entre ", detalle$Variable_X, " y ", colnames(data)[variable_Y],
        " es ", detalle$Magnitud, " y ", signo, 
        "; esto implica que a mayor puntaje en ", detalle$Variable_X, 
        ", la persona suele tener ", ifelse(signo == "directa", "mayores", "menores"),
        " puntajes en ", colnames(data)[variable_Y], ".\n"
      )
      texto <- paste(texto, interpretacion)
    }
  }
  
  # Imprimir la tabla en formato Markdown y las interpretaciones
  cat(markdown_tabla, "\n")
  cat(texto, "\n")
}

Necesito que hagas los siguientes ajustes: cuando la magnitud del índice rho sea nula, auqnue sea estadísticamente significativa:

- Añade la frase siguiente justo después de mencionar las correlaciones no estadísticamente significativas: "Por otro lado, la(s) siguiente(s) correlaciones, si bien técnicamente significativa(s), son de magnitud tan pequeño que no indican ninguna asociación en la práctica: {menciona el nombre de la(s) variable(s) independiente(s) del caso, separadas por comas}", 
- No la incluya esas correlaciones en los subsiguientes párrafos de interpretación. 

Asimismo, la división en párrafos para la interpretación de cada índice rho ha dado resultado. Mejórala intercalando puntos aparte y añadiendo una viñeta o bullet point al principio de cada interpretación.

Me puedes hacer todas las preguntas que consideres necesarias para dar tu mejor resultado.

## Prompt para Dunn Test

Crea una función que ejecute un Dunn Test. Los parámetros deben ser el dataframe, y las variables de datos y grupo, identificadas con el número de columna.

El output o salida debe llevar como título "Análisis Dunn de las diferencias estadísticamente significativas" y debe ser una matriz de doble entrada, donde las filas deben ser la(s) variables de datos, y las columnas deben ser como sigue: 

1. La primera columna debe llamarse "Significación" y debe contener el nivel de significancia estadística del p-valor (según lo usual: ***,  **, *, NS)
2. La segunda columna debe llamarse "Diferencia Real Probable", y debe contener los valores "Sí" (si el p-valor es menor a 0.05) o "No" si el p-valor es igual o mayor a 0.05.

Me puedes hacer todas las preguntas que consideres necesarias para dar tu mejor resultado.

## Prompt para seleccionar data numérica destinada a una TED

En el contexto del lenguaje de programación estadística R, necesito que me generes una función que me permita:

1. Seleccionar una serie de variables numéricas a partir de un dataframe
1. Filtrar todos los casos de "NA" que aparezcan (eliminarlos) para cualesquiera de las variables.
1. Los prompts deben ser: 
  * El nombre del dataframe
  * Las columnas en donde están las variables (identificadas por el número de columna)

Me puedes hacer todas las preguntas que consideres necesarias para dar tu mejor resultado.

## Prompt para boxplot

Necesito que crees una función que genere gráficos boxplot orientados en formato horizontal ("echados"). La función debe ser capaz de generar boxplots uno debajo del otro a fin de comparar las distribuciones de las variables. Cada boxplot deberá estar coloreado según la paleta RColorBrewer que yo escoja, siendo el valor de paleta por defecto "Dark2".

Los prompts para la función deben ser: 

* El nombre del dataframe
* El rango de columnas numéricas a graficar.

Me puedes hacer todas las preguntas que consideres necesarias para dar tu mejor resultado.

Etiquetas de los ejes:

    ¿Prefieres que las etiquetas de cada boxplot sean los nombres de las columnas seleccionadas? Sí
    ¿Quieres personalizar el título del gráfico o las etiquetas de los ejes, o dejo valores por defecto (e.g., "Boxplots de las Variables")? Añade un prompt que me permita definir cuál será el título

Personalización del diseño:

    ¿Debería ajustarse automáticamente el tamaño de la figura en función del número de variables seleccionadas? Sí
    ¿Necesitas incluir una leyenda o alguna información adicional en el gráfico? No

Paleta de colores:

    La paleta predeterminada será "Dark2". ¿Quieres que la función valide si la paleta especificada existe en RColorBrewer y genere un mensaje de error si no es válida? No
    
## Para AFC

Necesito generar una función que me permita ejecutar un análisis factorial confirmatorio (en adelante "AFC") usando el paquete lavaan, según lo siguiente:

* Los argumentos deben ser: el nombre del dataframe, el nombre de cada factor, los indicadores para cada factor y el nombre del objeto R que contendrá los resultados. 
* El dataframe no contendrá valores perdidos y estará compuesto exclusivamente por variables numéricos en columnas. De eso me encargo yo.
* El argumento "nombre de factor" estará compuesto por un vector compuesto por cadenas de caracteres. Por ejemplo: c("Factor1", "Factor 2", "Factor 3")
* El argumento "indicadores de cada factor" estará compuesto por un vector para cada factor, el cual contendrá el número que identifica a las columnas que componen el factor. El vector contendrá los números que identifican a las variables. Por ejemplo, para el factor 1 se denotaría por c(1:10); para el factor 2 se denotaría c(11:20), etc.
* El argumento "nombre del objeto R que contendrá los resultados" estará definido como una cadena de caracteres que yo definiré cada vez.

Me puedes hacer todas las preguntas que consideres necesarias para dar tu mejor resultado.

### Responses

1. Sí
2. Sí
3. Sí, quiero los resultados estandarizados.

## Results for CFA


Necesito que tomes esta función:

AFC <- function(data, factors, indicators, result_object_name) {
  # Verificar que la longitud de `factors` coincida con la longitud de `indicators`
  if (length(factors) != length(indicators)) {
    stop("El número de factores debe coincidir con el número de listas de indicadores.")
  }
  
  # Generar la sintaxis del modelo automáticamente
  modelo <- ""
  for (i in seq_along(factors)) {
    factor_name <- factors[i]
    column_indices <- indicators[[i]]
    
    # Convertir los índices a nombres de las columnas
    variable_names <- colnames(data)[column_indices]
    variables <- paste(variable_names, collapse = " + ")
    modelo <- paste0(modelo, factor_name, " =~ ", variables, "\n")
  }
  
  # Ajustar el modelo usando lavaan::cfa()
  ajuste <- cfa(modelo, data = data)
  
  # Mostrar un resumen de resultados con índices de ajuste y resultados estandarizados
  summary(ajuste, fit.measures = TRUE, standardized = TRUE)
  
  # Guardar el objeto con los resultados en el entorno global
  assign(result_object_name, ajuste, envir = .GlobalEnv)
  
  # Retornar el objeto de ajuste
  return(ajuste)
}

Y la modifiques para añadir un aergumento adicional, llamado "Nombre de la escala", el cual será una cadena de texto que yo definiré con la estructura: c("Nombre de la escala"). 

Además, ahora el resultado final de la función ahora deberá ser una tabla titulada **Resumen de Índices de Bondad de Ajuste para el AFC de la {Nombre de la Escala}**, en rMarkdown formateable con kable, según lo siguiente:

| **Índice**   |**Índice Obtenido**| **Nivel Obtenido**|
|--------------|-------------------|-------------------|
| RMSEA        |  (valor)          | (evaluación)      |
| CFI          |  (valor)          | (evaluación)      |
| TLI/NNFI     |  (valor)          | (evaluación)      |
| SRMR         |  (valor)          | (evaluación)      |
| GFI          |  (valor)          | (evaluación)      |


Para definir qué poner en la columna "Nivel Obtenido", usa la tabla siguiente como guía:

| **Índice**   | **Nivel Excelente**       | **Nivel Aceptable**    |
|--------------|---------------------------|------------------------|
| RMSEA        | ≤ 0.05                    | 0.05–0.08              |
| CFI          | ≥ 0.95                    | ≥ 0.90                 |
| TLI/NNFI     | ≥ 0.95                    | ≥ 0.90                 |
| SRMR         | ≤ 0.08                    | ≤ 0.10                 |
| GFI          | ≥ 0.95                    | ≥ 0.90                 |

Me puedes hacer todas las preguntas que consideres necesarias para dar tu mejor resultado.

### CFA p3

En la función siguiente:

AFC <- function(data, factors, indicators, result_object_name, scale_name) {
  # Verificar que la longitud de `factors` coincida con la longitud de `indicators`
  if (length(factors) != length(indicators)) {
    stop("El número de factores debe coincidir con el número de listas de indicadores.")
  }

  # Generar la sintaxis del modelo automáticamente
  modelo <- ""
  for (i in seq_along(factors)) {
    factor_name <- factors[i]
    column_indices <- indicators[[i]]

    # Convertir los índices a nombres de las columnas
    variable_names <- colnames(data)[column_indices]
    variables <- paste(variable_names, collapse = " + ")
    modelo <- paste0(modelo, factor_name, " =~ ", variables, "\n")
  }

  # Ajustar el modelo usando lavaan::cfa()
  ajuste <- cfa(modelo, data = data)

  # Obtener índices de ajuste relevantes
  fit_indices <- fitmeasures(ajuste, c("rmsea", "cfi", "tli", "srmr", "gfi"))

  # Función auxiliar para evaluar el nivel obtenido
  evaluar_indice <- function(indice, valor) {
    case_when(
      indice == "rmsea" & valor <= 0.05 ~ "Excelente",
      indice == "rmsea" & valor <= 0.08 ~ "Aceptable",
      indice == "cfi" & valor >= 0.95 ~ "Excelente",
      indice == "cfi" & valor >= 0.90 ~ "Aceptable",
      indice == "tli" & valor >= 0.95 ~ "Excelente",
      indice == "tli" & valor >= 0.90 ~ "Aceptable",
      indice == "srmr" & valor <= 0.08 ~ "Excelente",
      indice == "srmr" & valor <= 0.10 ~ "Aceptable",
      indice == "gfi" & valor >= 0.95 ~ "Excelente",
      indice == "gfi" & valor >= 0.90 ~ "Aceptable",
      TRUE ~ "Deficiente"
    )
  }

  # Crear tabla con resultados
  resultados_tabla <- data.frame(
    "Índice" = c("RMSEA", "CFI", "TLI/NNFI", "SRMR", "GFI"),
    "Índice Obtenido" = c(fit_indices["rmsea"], 
                          fit_indices["cfi"], 
                          fit_indices["tli"], 
                          fit_indices["srmr"], 
                          fit_indices["gfi"]),
    "Nivel Obtenido" = c(
      evaluar_indice("rmsea", fit_indices["rmsea"]),
      evaluar_indice("cfi", fit_indices["cfi"]),
      evaluar_indice("tli", fit_indices["tli"]),
      evaluar_indice("srmr", fit_indices["srmr"]),
      evaluar_indice("gfi", fit_indices["gfi"])
    )
  )

  # Guardar el objeto con los resultados en el entorno global
  assign(result_object_name, ajuste, envir = .GlobalEnv)

  # Mostrar título y tabla formateada
  cat("\n### Resumen de Índices de Bondad de Ajuste para el AFC de la", scale_name, "\n\n")
  print(kable(resultados_tabla, align = "c", caption = paste("Resumen de Índices de Bondad de Ajuste para el AFC de la", scale_name)))
}

La tabla final resultante contiene una columna inicial en la que se repiten los nombres de los índices de ajuste. ¿Puedes reformular la función para que esa columna inicial no se muestre?



### CFA p4

A partir de la función siguiente:

AFC <- function(data, factors, indicators, result_object_name, scale_name) {
  # Verificar que la longitud de `factors` coincida con la longitud de `indicators`
  if (length(factors) != length(indicators)) {
    stop("El número de factores debe coincidir con el número de listas de indicadores.")
  }

  # Generar la sintaxis del modelo automáticamente
  modelo <- ""
  for (i in seq_along(factors)) {
    factor_name <- factors[i]
    column_indices <- indicators[[i]]

    # Convertir los índices a nombres de las columnas
    variable_names <- colnames(data)[column_indices]
    variables <- paste(variable_names, collapse = " + ")
    modelo <- paste0(modelo, factor_name, " =~ ", variables, "\n")
  }

  # Ajustar el modelo usando lavaan::cfa()
  ajuste <- cfa(modelo, data = data)

  # Obtener índices de ajuste relevantes
  fit_indices <- fitmeasures(ajuste, c("rmsea", "cfi", "tli", "srmr", "gfi"))

  # Función auxiliar para evaluar el nivel obtenido
  evaluar_indice <- function(indice, valor) {
    case_when(
      indice == "rmsea" & valor <= 0.05 ~ "Excelente",
      indice == "rmsea" & valor <= 0.08 ~ "Aceptable",
      indice == "cfi" & valor >= 0.95 ~ "Excelente",
      indice == "cfi" & valor >= 0.90 ~ "Aceptable",
      indice == "tli" & valor >= 0.95 ~ "Excelente",
      indice == "tli" & valor >= 0.90 ~ "Aceptable",
      indice == "srmr" & valor <= 0.08 ~ "Excelente",
      indice == "srmr" & valor <= 0.10 ~ "Aceptable",
      indice == "gfi" & valor >= 0.95 ~ "Excelente",
      indice == "gfi" & valor >= 0.90 ~ "Aceptable",
      TRUE ~ "Deficiente"
    )
  }

  # Crear tabla con resultados (sin la columna con nombres de índices)
  resultados_tabla <- data.frame(
    "Índice Obtenido" = c(fit_indices["rmsea"], 
                          fit_indices["cfi"], 
                          fit_indices["tli"], 
                          fit_indices["srmr"], 
                          fit_indices["gfi"]),
    "Nivel Obtenido" = c(
      evaluar_indice("rmsea", fit_indices["rmsea"]),
      evaluar_indice("cfi", fit_indices["cfi"]),
      evaluar_indice("tli", fit_indices["tli"]),
      evaluar_indice("srmr", fit_indices["srmr"]),
      evaluar_indice("gfi", fit_indices["gfi"])
    )
  )

  # Cambiar nombres de las filas para usar como identificadores
  rownames(resultados_tabla) <- c("RMSEA", "CFI", "TLI/NNFI", "SRMR", "GFI")

  # Guardar el objeto con los resultados en el entorno global
  assign(result_object_name, ajuste, envir = .GlobalEnv)

  # Mostrar título y tabla formateada
  cat("\n### Resumen de Índices de Bondad de Ajuste para el AFC de la", scale_name, "\n\n")
  print(kable(resultados_tabla, align = "c", 
              caption = paste("Resumen de Índices de Bondad de Ajuste para el AFC de la", scale_name)))
}

Añade al resultado de la función una matriz de residuales estandarizados. La tabla adicional de la que hablo deberá llevar por título "Matriz de Residuales estandarizados Escala #", donde el nombre de la escala provendrá del argumento "scale_name" de la función. Rebautiza la función como "AFC2". La tabla adicional deberá ser editable con kable.

Hazme las preguntas que consideres necesarias para darme el mejor resultado posible.


1. 
* Limítate a resaltar únicamente los residuales significativos.
* ¿La tabla debe incluir formateos específicos, como colores para valores altos o bajos? Sí, negritas para valores altos, y cursiva para valores bajos.
2.
* ¿Quieres que la matriz se imprima directamente con kable, o la incluyo como parte de un objeto de salida que puedas inspeccionar y guardar? Imprímelo directamente con kable
* ¿Debe la matriz de residuales ser devuelta como parte de una lista junto con los índices de ajuste y el modelo ajustado? No
3. 
* Actualmente el modelo ajustado se guarda en el entorno global con assign. ¿Quieres mantener este comportamiento o prefieres que la función devuelva una lista con todos los elementos (modelo, índices y matriz de residuales)? Mejor guádalo como una lista tal como describes.
4. 
* Si la escala tiene muchos ítems, la matriz de residuales puede volverse grande. ¿Debo limitar la visualización en pantalla a ciertos ítems clave o dejarla completa? Limita la visualización en pantalla a ciertos ítems clave
5. 
* ¿Debo recalcular los índices de ajuste después de inspeccionar los residuales (por ejemplo, si planeas modificar el modelo según los residuales)? No


# Outliers01

Créame una función usando cñodigo R. La función deberá llamarse "intensos", y deberá hacer lo siguiente:

1. A partir de un data frame compuesto por una serie de variables en columnas y casos en filas, donde cada columna es un ítem, detectar qué casos constituyen outliers o valores atípicos. 


# Outliers02

Tomando la función siguiente como punto de partida:

items_intensos <- function(data, rango_columnas, columna_contexto, multiplicador_IQR = 1.5) {
  # Verificar que las columnas estén en el rango
  columnas_seleccionadas <- names(data)[rango_columnas]
  
  # Identificar columnas no numéricas
  columnas_no_numericas <- columnas_seleccionadas[!sapply(data[, rango_columnas, drop = FALSE], is.numeric)]
  
  # Excluir las columnas no numéricas y avisar
  columnas_a_evaluar <- setdiff(columnas_seleccionadas, columnas_no_numericas)
  if (length(columnas_no_numericas) > 0) {
    cat("Las siguientes columnas no son numéricas y serán excluidas del análisis:\n")
    print(columnas_no_numericas)
  }
  
  # Inicializar una tabla consolidada
  resultados_outliers <- data.frame(
    Columna = character(),
    Fila = integer(),
    Valor_Outlier = numeric(),
    Contexto = character()
  )
  
  # Aplicar la detección de outliers para cada columna numérica
  for (columna in columnas_a_evaluar) {
    # Obtener los datos de la columna
    valores <- data[[columna]]
    
    # Calcular los cuartiles y los límites del IQR
    Q1 <- quantile(valores, 0.25, na.rm = TRUE)
    Q3 <- quantile(valores, 0.75, na.rm = TRUE)
    IQR <- Q3 - Q1
    limite_inferior <- Q1 - multiplicador_IQR * IQR
    limite_superior <- Q3 + multiplicador_IQR * IQR
    
    # Identificar las filas con outliers
    filas_outliers <- which(valores < limite_inferior | valores > limite_superior)
    
    # Crear un data frame con los resultados para esta columna
    if (length(filas_outliers) > 0) {
      resultados <- data.frame(
        Columna = columna,
        Fila = filas_outliers,
        Valor_Outlier = valores[filas_outliers],
        Contexto = data[[columna_contexto]][filas_outliers]
      )
      
      # Consolidar los resultados
      resultados_outliers <- rbind(resultados_outliers, resultados)
    }
  }
  
  # Generar una tabla en Markdown
  if (nrow(resultados_outliers) > 0) {
    cat("### Tabla consolidada de casos con valores atípicos\n")
    kable(resultados_outliers, format = "markdown", row.names = FALSE)
  } else {
    cat("No se detectaron valores atípicos en las columnas evaluadas.\n")
  }
}

Genera una función alternativa, llamada items_intensos_resumen, que genere:

1. Una matriz de doble entrada donde las filas sean los valores con la sumatoria de cuántos casos atípicos existen para cada columna o ítem, de la forma siguiente:
|Ítem    | Valor_Outlier|Frecuencia |
|:-------|-------------:|:----------|
|S6      |             6|          7|
|S6      |             7|          4|
|S7      |             6|          3|
|S79     |             0|          2|

2. Una tabla de frecuencias descendentes del campo "Contexto" con la sumatoria de cuántos casos existen para cada valor diferente, por ejemplo:

|Contexto| Frecuencia|
|:-------|----------:|
|Costa   |          9| 
|Sierra  |          7| 
|Selva   |          6|


Ambas tablas deben ser editables en kable para Markdown.

# Outliers3

Quiero que hagas una versión alternativa de la función siguiente, para que solo muestre la la tabla "Tabla de Outliers según Categoría":

items_intensos_resumen <- function(data, rango_columnas, columna_contexto, multiplicador_IQR = 1.5) {
  # Verificar las columnas numéricas dentro del rango
  columnas_seleccionadas <- names(data)[rango_columnas]
  columnas_no_numericas <- columnas_seleccionadas[!sapply(data[, rango_columnas, drop = FALSE], is.numeric)]
  columnas_a_evaluar <- setdiff(columnas_seleccionadas, columnas_no_numericas)
  
  if (length(columnas_no_numericas) > 0) {
    cat("Las siguientes columnas no son numéricas y serán excluidas del análisis:\n")
    print(columnas_no_numericas)
  }
  
  # Crear un data frame vacío para registrar los outliers
  registros_outliers <- data.frame(
    Columna = character(),
    Valor_Outlier = numeric(),
    Contexto = character()
  )
  
  # Identificar los outliers en cada columna numérica
  for (columna in columnas_a_evaluar) {
    valores <- data[[columna]]
    Q1 <- quantile(valores, 0.25, na.rm = TRUE)
    Q3 <- quantile(valores, 0.75, na.rm = TRUE)
    IQR <- Q3 - Q1
    limite_inferior <- Q1 - multiplicador_IQR * IQR
    limite_superior <- Q3 + multiplicador_IQR * IQR
    filas_outliers <- which(valores < limite_inferior | valores > limite_superior)
    
    if (length(filas_outliers) > 0) {
      registros <- data.frame(
        Columna = columna,
        Valor_Outlier = valores[filas_outliers],
        Contexto = data[[columna_contexto]][filas_outliers]
      )
      registros_outliers <- bind_rows(registros_outliers, registros)
    }
  }
  
  if (nrow(registros_outliers) > 0) {
    # Matriz de doble entrada: sumar frecuencia por ítem y valor atípico
    matriz_doble_entrada <- registros_outliers %>%
      group_by(Columna, Valor_Outlier) %>%
      summarise(Frecuencia = n(), .groups = "drop")
    
    # Tabla de frecuencias descendentes del contexto
    tabla_frecuencias <- registros_outliers %>%
      count(Contexto, sort = TRUE, name = "Frecuencia")
    
    # Imprimir tablas usando kable
    cat("\n### Resumen de Outliers por ítem\n")
    print(kable(matriz_doble_entrada, format = "markdown", row.names = FALSE))
    
    cat("\n### Tabla de Outliers según Categoría\n")
    print(kable(tabla_frecuencias, format = "markdown", row.names = FALSE))
  } else {
    cat("No se detectaron valores atípicos en las columnas evaluadas.\n")
  }
}

La nueva función deberá llamarse items_intensos_catego


## Función Resumen Binarios

Quiero que corrijas la función siguiente:

resumen_binario <- function(data, columnas, equivalencias = NULL) {
  library(dplyr)
  library(knitr)
  
  # Paso 1: Verificar y transformar equivalencias si es necesario
  if (!is.null(equivalencias)) {
    cat("Transformando las respuestas según las equivalencias proporcionadas...\n")
    for (columna in columnas) {
      data[[columna]] <- recode(data[[columna]],
                                !!equivalencias[1] := "Sí",
                                !!equivalencias[2] := "No")
    }
  } else {
    # Validar que las columnas contengan "Sí" y "No"
    valores_distintos <- unique(unlist(data[columnas]))
    if (!all(valores_distintos %in% c("Sí", "No"))) {
      stop("Las columnas contienen valores distintos de 'Sí' y 'No'. Por favor, especifica equivalencias.")
    }
  }
  
  # Paso 2: Calcular las frecuencias y porcentajes de "No" para cada columna
  resultados <- data %>%
    select(all_of(columnas)) %>%
    summarise(across(everything(), ~ sum(. == "No"), .names = "Frecuencia_{col}")) %>%
    pivot_longer(everything(), names_to = "Ítem", values_to = "Frecuencia") %>%
    mutate(Ítem = gsub("Frecuencia_", "", Ítem),  # Limpiar nombres de columnas
           Porcentaje = round((Frecuencia / nrow(data)) * 100, 1)) %>%
    arrange(desc(Frecuencia))  # Ordenar en orden descendente
  
  # Paso 3: Formatear y mostrar la tabla
  cat("\n### Tabla de Frecuencias y Porcentajes de Respuestas 'No'\n")
  print(kable(resultados, format = "markdown", row.names = FALSE))
  
  return(resultados)
}

De tal modo que:

1. No arroje un tibble, sino solo la tabla en Markdown
2. Genere además un gráfico de barras en orden descendente.

Para el gráfico de barras puedes inspirarte en esta función que te copio aquí:

graficar_porcentajes <- function(variable, data, 
                                 titulo = "Distribución porcentual", 
                                 etiqueta_x = "Categorías", 
                                 etiqueta_y = "Porcentaje", 
                                 pie_de_pagina = "Fuente: Datos internos. Elaboración propia", 
                                 paleta_colores = "Dark2") {
  # Validar que la variable es un factor
  if (!is.factor(data[[deparse(substitute(variable))]])) {
    stop("La variable debe ser de tipo factor. Por favor, conviértela antes de usar esta función.")
  }
  
  # Calcular los porcentajes
  datos_porcentajes <- data %>%
    count({{ variable }}) %>%
    mutate(porcentaje = n / sum(n) * 100)
  
  # Redondear los porcentajes
  datos_porcentajes$porcentaje <- round(datos_porcentajes$porcentaje, 2)
  
  # Crear el gráfico de barras con porcentajes
  ggplot(datos_porcentajes, aes(x = {{ variable }}, y = porcentaje, fill = {{ variable }})) +
    geom_col(width = 0.75) +
    geom_text(aes(label = paste0(porcentaje, " %")), vjust = 1.2) +
    scale_fill_brewer(palette = paleta_colores) +
    labs(
      title = titulo,
      x = etiqueta_x,
      y = etiqueta_y,
      caption = pie_de_pagina
    ) +
    theme_minimal() +
    theme(legend.position = "none")
}


Puedes adaptar la función de gráfico de barras como te parezca más eficiente.

Puedes hacerme las preguntas que consideres necesarias para generar tu mejor resultado.

## Corección:

Corrige la función siguiente:

resumen_binario <- function(data, columnas, equivalencias = NULL, 
                            titulo_grafico = "Distribución de respuestas 'Sí'",
                            etiqueta_x = "Ítems", 
                            etiqueta_y = "Frecuencia", 
                            pie_de_pagina = "Fuente: Nivel 4-5", 
                            paleta_colores = "Dark2") {
  library(dplyr)
  library(knitr)
  library(ggplot2)
  
  # Paso 1: Verificar y transformar equivalencias si es necesario
  if (!is.null(equivalencias)) {
    cat("Transformando las respuestas según las equivalencias proporcionadas...\n")
    for (columna in columnas) {
      data[[columna]] <- recode(data[[columna]],
                                !!equivalencias[1] := "Sí",
                                !!equivalencias[2] := "No")
    }
  } else {
    # Validar que las columnas contengan "Sí" y "No"
    valores_distintos <- unique(unlist(data[columnas]))
    if (!all(valores_distintos %in% c("Sí", "No"))) {
      stop("Las columnas contienen valores distintos de 'Sí' y 'No'. Por favor, especifica equivalencias.")
    }
  }
  
  # Paso 2: Calcular las frecuencias y porcentajes de "No" para cada columna
  resultados <- data %>%
    select(all_of(columnas)) %>%
    summarise(across(everything(), ~ sum(. == "No"), .names = "Frecuencia_{col}")) %>%
    pivot_longer(everything(), names_to = "Ítem", values_to = "Frecuencia") %>%
    mutate(Ítem = gsub("Frecuencia_", "", Ítem),  # Limpiar nombres de columnas
           Porcentaje = round((Frecuencia / nrow(data)) * 100, 1)) %>%
    arrange(desc(Frecuencia))  # Ordenar en orden descendente
  
  # Paso 3: Formatear y mostrar la tabla
  cat("Tabla de Frecuencias y Porcentajes de Respuestas 'Sí'\n")
  print(kable(resultados, format = "markdown", row.names = FALSE))
  
  # Paso 4: Crear el gráfico de barras
  grafico <- ggplot(resultados, aes(x = reorder(Ítem, -Frecuencia), y = Frecuencia, fill = Ítem)) +
    geom_col(width = 0.75) +
    geom_text(aes(label = paste0(Frecuencia, " (", Porcentaje, "%)")), 
              vjust = -0.5, size = 3) +
    scale_fill_brewer(palette = paleta_colores) +
    labs(
      title = titulo_grafico,
      x = etiqueta_x,
      y = etiqueta_y,
      caption = pie_de_pagina
    ) +
    theme_minimal() +
    theme(
      legend.position = "none",
      axis.text.x = element_text(angle = 45, hjust = 1)
    )
  
  # Mostrar el gráfico
  print(grafico)
}

Para asegurarte de que:
1. Los colores de las barras provengan de una paleta de RColorBrewer, y establece el nombre de la paleta a utilizar como un argumento de la función.
2. El eje Y del gráfico de barras muestre una escala porcentual en vez de la frecuencia absoluta.

Puedes hacerme las preguntas que consideres necesarias para generar tu mejor resultado.

### Corección

Coge esta función:

resumen_binario <- function(data, columnas, equivalencias = NULL, 
                            titulo_grafico = "Distribución de respuestas 'Sí'",
                            etiqueta_x = "Ítems", 
                            etiqueta_y = "Porcentaje", 
                            pie_de_pagina = "Fuente: Nivel 4-5", 
                            paleta_colores = "Dark2") {
  library(dplyr)
  library(knitr)
  library(ggplot2)
  library(tidyr)  # Para pivot_longer
  library(RColorBrewer)  # Para validar la paleta de colores
  
  # Validar que la paleta de colores exista en RColorBrewer
  if (!paleta_colores %in% rownames(brewer.pal.info)) {
    stop(paste("La paleta de colores", paleta_colores, "no es válida. Por favor, elige una de las paletas disponibles en RColorBrewer."))
  }
  
  # Paso 1: Verificar y transformar equivalencias si es necesario
  if (!is.null(equivalencias)) {
    cat("Transformando las respuestas según las equivalencias proporcionadas...\n")
    for (columna in columnas) {
      data[[columna]] <- recode(data[[columna]],
                                !!equivalencias[1] := "Sí",
                                !!equivalencias[2] := "No")
    }
  } else {
    # Validar que las columnas contengan "Sí" y "No"
    valores_distintos <- unique(unlist(data[columnas]))
    if (!all(valores_distintos %in% c("Sí", "No"))) {
      stop("Las columnas contienen valores distintos de 'Sí' y 'No'. Por favor, especifica equivalencias.")
    }
  }
  
  # Paso 2: Calcular las frecuencias y porcentajes de "Sí" para cada columna
  resultados <- data %>%
    select(all_of(columnas)) %>%
    summarise(across(everything(), ~ sum(. == "Sí"), .names = "Frecuencia_{col}")) %>%
    pivot_longer(everything(), names_to = "Ítem", values_to = "Frecuencia") %>%
    mutate(Ítem = gsub("Frecuencia_", "", Ítem),  # Limpiar nombres de columnas
           Porcentaje = round((Frecuencia / nrow(data)) * 100, 1)) %>%
    arrange(desc(Porcentaje))  # Ordenar en orden descendente por porcentaje
  
  # Paso 3: Formatear y mostrar la tabla
  cat("Tabla de Frecuencias y Porcentajes de Respuestas 'Sí'\n")
  print(kable(resultados, format = "markdown", row.names = FALSE))
  
  # Paso 4: Crear el gráfico de barras con escala porcentual
  grafico <- ggplot(resultados, aes(x = reorder(Ítem, -Porcentaje), y = Porcentaje, fill = Ítem)) +
    geom_col(width = 0.75) +
    geom_text(aes(label = paste0(Porcentaje, "%")), 
              vjust = -0.5, size = 3) +
    scale_y_continuous(labels = scales::percent_format(scale = 1)) +  # Escala de porcentaje en el eje Y
    scale_fill_brewer(palette = paleta_colores) +
    labs(
      title = titulo_grafico,
      x = etiqueta_x,
      y = etiqueta_y,
      caption = pie_de_pagina
    ) +
    theme_gray() +
    theme(
      legend.position = "none",
      axis.text.x = element_text(angle = 45, hjust = 1)
    )
  
  # Mostrar el gráfico
  print(grafico)
}
Y añade al gráfico de barras una línea o borde a las barras. Define el color del borde de las barras como un argumento. 
#### Corección

Ahora nevcesito que ajustes la función siguiente:

anal_di <- function(data, rango_indices) {
  # Seleccionar las variables según el rango
  sub_data <- data[, rango_indices]
  
  # Verificar que las columnas sean dicotómicas
  if (!all(apply(sub_data, 2, function(x) all(x %in% c(0, 1))))) {
    stop("Todas las columnas seleccionadas deben ser dicotómicas (valores 0 y 1).")
  }
  
  # Calcular los porcentajes de "1" para cada variable
  tabla_porcentajes <- sub_data %>%
    summarise(across(everything(), ~ mean(.x) * 100)) %>%
    pivot_longer(cols = everything(), names_to = "Variable", values_to = "Porcentaje") %>%
    arrange(desc(Porcentaje))
  
  # Imprimir la tabla de frecuencias
  print(tabla_porcentajes)
  
  # Crear el gráfico de barras
  grafico <- ggplot(tabla_porcentajes, aes(x = reorder(Variable, -Porcentaje), y = Porcentaje, fill = Variable)) +
    geom_col(width = 0.75) +
    geom_text(aes(label = paste0(round(Porcentaje, 1), "%")), vjust = -0.5) +
    scale_fill_brewer(palette = "Dark2") +
    labs(
      title = "Porcentajes de 'Sí' (1) por variable",
      x = "Variables",
      y = "Porcentaje",
      caption = "Fuente: Datos internos. Elaboración propia"
    ) +
    theme_minimal() +
    theme(legend.position = "none")
  
  print(grafico)
}

Para añadir lo siguiente:

a. La tabla deberá ser en formato Markdown, editable con kable
b. En el gráfico, debes añadir una línes de contorno de color para las barras, y el tema de ggplot2 debe ser theme_gray; asimismo, deberás usar una paleta de RColorBrewer que yo te voy a definir.
c. Igualmente, debes añadir lo siguiente como argumentos de la función:
c.1. El título de la tabla
c.2. El título del Gráfico
c.3. El color de la línea de contorno
c.4. La paleta de RColorBrewer para el relleno de las barras

####Corección

Partiendo de la función siguiente, escrita en el lenguaje estadístico R, que ejecuta la transformación logarítmica de una escala:

log_transform_norma <- function(data) {
  # Verificar que las columnas sean numéricas
  data <- data[, sapply(data, is.numeric)]
  
  # Aplicar la transformación logarítmica (con manejo de valores <= 0)
  log_transformada <- as.data.frame(lapply(data, function(x) {
    if (any(x <= 0)) return(rep(NA, length(x))) else return(log(x))
  }))
  
  # Evaluar normalidad con Shapiro-Wilk y Lilliefors
  evaluar_normalidad <- function(variable) {
    if (length(variable[!is.na(variable)]) > 2) {
      shapiro_p <- tryCatch(shapiro.test(variable)$p.value, error = function(e) NA)
      ks_p <- tryCatch(nortest::lillie.test(variable)$p.value, error = function(e) NA)
    } else {
      shapiro_p <- ks_p <- NA
    }
    list(Shapiro_Wilk = shapiro_p, Lilliefors_KS = ks_p)
  }
  
  # Crear un dataframe para almacenar los resultados
  resultados <- data.frame(
    Escala = character(),
    Índice = character(),
    `p-valor` = character(),
    Interpretación = character(),
    stringsAsFactors = FALSE
  )
  
  interpretar_resultado <- function(p) {
    if (is.na(p)) return(c("NA", "Valor insuficiente"))
    nivel_significancia <- ifelse(p <= 0.001, "***",
                                  ifelse(p <= 0.01, "**",
                                         ifelse(p <= 0.05, "*", "NS")))
    interpretacion <- ifelse(p > 0.05, "Normalidad", "No Normalidad")
    c(sprintf("%.2f, %s", p, nivel_significancia), interpretacion)
  }
  
  # Iterar sobre cada columna
  for (var in colnames(log_transformada)) {
    variable <- log_transformada[[var]]
    resultados_test <- evaluar_normalidad(variable)
    
    # Agregar resultados de Shapiro-Wilk
    sw_resultado <- interpretar_resultado(resultados_test$Shapiro_Wilk)
    resultados <- rbind(resultados, data.frame(
      Escala = var,
      Índice = "S-W",
      `p-valor` = sw_resultado[1],
      Interpretación = sw_resultado[2],
      stringsAsFactors = FALSE
    ))
    
    # Agregar resultados de Lilliefors
    ks_resultado <- interpretar_resultado(resultados_test$Lilliefors_KS)
    resultados <- rbind(resultados, data.frame(
      Escala = var,
      Índice = "K-S, L",
      `p-valor` = ks_resultado[1],
      Interpretación = ks_resultado[2],
      stringsAsFactors = FALSE
    ))
  }
  
  # Mostrar resultados como tabla
  kable(resultados, format = "markdown", caption = "Transformación Logarítmica de las Escalas")
}

Crea una función llamada box_norma, que haga exactamente lo mismo, pero en vez de una transformación logarítmica, que ejecute una transformación Box-Cox.
# Tranformaciones

Crea una función que me permita generar la transformación logarítmica de una serie de escalas, y luego evalúe la normalidad de la distribuciones transformadas resultantes. Para evaluar e interpretar los resultados de los tests de normalidad para las distribuciones transformadas resultantes, puedes basarte en la siguiente función:

Norma <- function(data) {
  # Función para calcular el nivel de significancia y formatear el resultado
  interpretar_resultado <- function(p) {
    if (is.na(p)) return("NA")
    nivel_significancia <- ifelse(p <= 0.001, "***",
                                  ifelse(p <= 0.01, "**",
                                         ifelse(p <= 0.05, "*", "NS")))
    interpretacion <- ifelse(p > 0.05, "Se puede asumir normalidad", "No se puede asumir normalidad")
    paste0("p = ", sprintf("%.2f", p), " ", nivel_significancia, " (", interpretacion, ")")
  }
  
  # Lista para almacenar resultados
  resultados <- list()
  
  # Iterar sobre cada columna del dataframe
  for (var in colnames(data)) {
    variable <- data[[var]]
    
    # Calcular los p-valores
    shapiro_p <- ifelse(length(variable) > 2, shapiro.test(variable)$p.value, NA)
    # ks_p <- ifelse(length(variable) > 1, ks.test(variable, "pnorm", mean(variable), sd(variable))$p.value, NA)
    ks_p <- ifelse(length(variable) > 1, nortest::lillie.test(variable)$p.value, NA)
    
    # ad_p <- ifelse(length(variable) > 1, ad.test(variable)$p.value, NA)
    
    # Interpretar y formatear los resultados
    shapiro_resultado <- interpretar_resultado(shapiro_p)
    ks_resultado <- interpretar_resultado(ks_p)
    # ad_resultado <- interpretar_resultado(ad_p)
    
    # Agregar resultados formateados a la lista
    resultados[[var]] <- c(
      Shapiro_Wilk = shapiro_resultado,
      Lilliefors_KS = ks_resultado
      # Anderson_Darling = ad_resultado
    )
  }
  
  # Convertir la lista en un dataframe
  resultados_df <- as.data.frame(do.call(rbind, resultados), stringsAsFactors = FALSE)
  rownames(resultados_df) <- colnames(data)
  
  # Personalizar la salida con kable
  resultados_df %>%
    kable(format = "markdown", align = "rll", caption = "Resultados de los tests de normalidad (S-W, Lilliefors K-S)")
}

El argumento de la función será el nombre del dataframe que contiene los ítems de la escala a transformar.

El resultado debe ser una tabla con los indices de Shapiro-Wilk y Kolmogorov-Smirnov (con corrección de Liliefors) para la(s) escalas transformadas.

La tabla debe tener la siguiente estructura: 

Table: Transformación Logarítmica de las Escalas

|Escala  |Índice        |p-valor | Interpretación |
|:-------|:-------------|:-------|:---------------|
|"C"     |S-W           |0.07, NS|No Normalidad   |
|"C"     |K-S, L        |0.08, NS|No Normalidad   |
|"S"     |S-W           |0.04**  |Normalidad      |
|"S"     |K-S, L        |0.06, NS|No Normalidad   |


### C02

Usando el lenguaje de programnación estadística R, modifica esta función, para que en vez de realizar una transformación logarítmica, reañice una transformación Box-Cox de la escala:

log_transform_norma <- function(data) {
  # Verificar que las columnas sean numéricas
  data <- data[, sapply(data, is.numeric)]
  
  # Aplicar la transformación logarítmica (con manejo de valores <= 0)
  log_transformada <- as.data.frame(lapply(data, function(x) {
    if (any(x <= 0)) return(rep(NA, length(x))) else return(log(x))
  }))
  
  # Evaluar normalidad con Shapiro-Wilk y Lilliefors
  evaluar_normalidad <- function(variable) {
    if (length(variable[!is.na(variable)]) > 2) {
      shapiro_p <- tryCatch(shapiro.test(variable)$p.value, error = function(e) NA)
      ks_p <- tryCatch(nortest::lillie.test(variable)$p.value, error = function(e) NA)
    } else {
      shapiro_p <- ks_p <- NA
    }
    list(Shapiro_Wilk = shapiro_p, Lilliefors_KS = ks_p)
  }
  
  # Crear un dataframe para almacenar los resultados
  resultados <- data.frame(
    Escala = character(),
    Índice = character(),
    `p-valor` = character(),
    Interpretación = character(),
    stringsAsFactors = FALSE
  )
  
  interpretar_resultado <- function(p) {
    if (is.na(p)) return(c("NA", "Valor insuficiente"))
    nivel_significancia <- ifelse(p <= 0.001, "***",
                                  ifelse(p <= 0.01, "**",
                                         ifelse(p <= 0.05, "*", "NS")))
    interpretacion <- ifelse(p > 0.05, "Normalidad", "No Normalidad")
    c(sprintf("%.2f, %s", p, nivel_significancia), interpretacion)
  }
  
  # Iterar sobre cada columna
  for (var in colnames(log_transformada)) {
    variable <- log_transformada[[var]]
    resultados_test <- evaluar_normalidad(variable)
    
    # Agregar resultados de Shapiro-Wilk
    sw_resultado <- interpretar_resultado(resultados_test$Shapiro_Wilk)
    resultados <- rbind(resultados, data.frame(
      Escala = var,
      Índice = "S-W",
      `p-valor` = sw_resultado[1],
      Interpretación = sw_resultado[2],
      stringsAsFactors = FALSE
    ))
    
    # Agregar resultados de Lilliefors
    ks_resultado <- interpretar_resultado(resultados_test$Lilliefors_KS)
    resultados <- rbind(resultados, data.frame(
      Escala = var,
      Índice = "K-S, L",
      `p-valor` = ks_resultado[1],
      Interpretación = ks_resultado[2],
      stringsAsFactors = FALSE
    ))
  }
  
  # Mostrar resultados como tabla
  kable(resultados, format = "markdown", caption = "Transformación Logarítmica de las Escalas")
}

### C03

El siguiente código

resumen_binario <- function(data, columnas, equivalencias = NULL, 
                            titulo_grafico = "Distribución de respuestas 'Sí'",
                            etiqueta_x = "Ítems", 
                            etiqueta_y = "Porcentaje", 
                            pie_de_pagina = "Fuente: Nivel 4-5", 
                            paleta_colores = "Dark2",
                            color_borde = "black") {  # Nuevo argumento para el color del borde
  library(dplyr)
  library(knitr)
  library(ggplot2)
  library(tidyr)  # Para pivot_longer
  library(RColorBrewer)  # Para validar la paleta de colores
  
  # Validar que la paleta de colores exista en RColorBrewer
  if (!paleta_colores %in% rownames(brewer.pal.info)) {
    stop(paste("La paleta de colores", paleta_colores, "no es válida. Por favor, elige una de las paletas disponibles en RColorBrewer."))
  }
  
# Paso 1: Verificar y transformar equivalencias si es necesario
if (!is.null(equivalencias)) {
  cat("Transformando las respuestas según las equivalencias proporcionadas...\n")
  for (columna in columnas) {
    data[[columna]] <- dplyr::recode(data[[columna]],
                                     equivalencias[1] : 'Sí',
                                     equivalencias[0] : 'No')
  }
} else {
  # Validar que las columnas contengan "Sí" y "No"
  valores_distintos <- unique(unlist(data[columnas]))
  if (!all(valores_distintos %in% c("Sí", "No"))) {
    stop("Las columnas contienen valores distintos de 'Sí' y 'No'. Por favor, especifica equivalencias.")
  }
}


  # Paso 2: Calcular las frecuencias y porcentajes de "Sí" para cada columna
  resultados <- data %>%
    select(all_of(columnas)) %>%
    summarise(across(everything(), ~ sum(. == "Sí"), .names = "Frecuencia_{col}")) %>%
    pivot_longer(everything(), names_to = "Ítem", values_to = "Frecuencia") %>%
    mutate(Ítem = gsub("Frecuencia_", "", Ítem),  # Limpiar nombres de columnas
           Porcentaje = round((Frecuencia / nrow(data)) * 100, 1)) %>%
    arrange(desc(Porcentaje))  # Ordenar en orden descendente por porcentaje
  
  # Paso 3: Formatear y mostrar la tabla
  cat("Tabla de Frecuencias y Porcentajes de Respuestas 'Sí'\n")
  print(kable(resultados, format = "markdown", row.names = FALSE))
  
  # Paso 4: Crear el gráfico de barras con escala porcentual y borde
  grafico <- ggplot(resultados, aes(x = reorder(Ítem, -Porcentaje), y = Porcentaje, fill = Ítem)) +
    geom_col(width = 0.75, color = color_borde) +  # Añadir borde a las barras
    geom_text(aes(label = paste0(Porcentaje, "%")), 
              vjust = -0.5, size = 3) +
    scale_y_continuous(labels = scales::percent_format(scale = 1)) +  # Escala de porcentaje en el eje Y
    scale_fill_brewer(palette = paleta_colores) +
    labs(
      title = titulo_grafico,
      x = etiqueta_x,
      y = etiqueta_y,
      caption = pie_de_pagina
    ) +
    theme_gray() +
    theme(
      legend.position = "none",
      axis.text.x = element_text(angle = 45, hjust = 1)
    )
  
  # Mostrar el gráfico
  print(grafico)
}



resumen_binario(INDI45, columnas = c(22:30), equivalencias = c("1","0"), 
                            titulo_grafico = "Distribución de Respuestas 'Sí'",
                            etiqueta_x = "Respuestas Sí", 
                            etiqueta_y = "Porcentaje", 
                            pie_de_pagina = "Fuente: Nivel 4-5", 
                            paleta_colores = "Oranges",
                            color_borde = "black")

Me arroja el siguiente error:

Error in equivalencias[1]:"Sí" : NA/NaN argument
  
  
  ¿Cuál es el problema?


###C04

En un dataframe, tengo una serie de variables cuyos valores son dicotómicos: "Sí" y "No". Necesito que me generes una función que:

1. Calcule el porcentaje de casos en los que el valor es "Sí". Por ejemplo, si en el dataframe tengo  10 casos y 2 personas tienen valor "1" en la variable "A", el porcentaje a mostrar para la variable "A" sería 20%, y así para el resto de variables dicotómicas. 
2. Genere una tabla de frecuencias descendentes donde se muestre el nombre de cada variable y el porcentaje de "1"s existentes.
3. Genere un gráfico de barras descendentes para todas las variables mostrando los porcentajes de la tabla. Para esto último, inspírate en la función siguiente:

graficar_porcentajes <- function(variable, data, 
                                 titulo = "Distribución porcentual", 
                                 etiqueta_x = "Categorías", 
                                 etiqueta_y = "Porcentaje", 
                                 pie_de_pagina = "Fuente: Datos internos. Elaboración propia", 
                                 paleta_colores = "Dark2") {
  # Validar que la variable es un factor
  if (!is.factor(data[[deparse(substitute(variable))]])) {
    stop("La variable debe ser de tipo factor. Por favor, conviértela antes de usar esta función.")
  }
  
  # Calcular los porcentajes
  datos_porcentajes <- data %>%
    count({{ variable }}) %>%
    mutate(porcentaje = n / sum(n) * 100)
  
  # Redondear los porcentajes
  datos_porcentajes$porcentaje <- round(datos_porcentajes$porcentaje, 2)
  
  # Crear el gráfico de barras con porcentajes
  ggplot(datos_porcentajes, aes(x = {{ variable }}, y = porcentaje, fill = {{ variable }})) +
    geom_col(width = 0.75) +
    geom_text(aes(label = paste0(porcentaje, " %")), vjust = 1.2) +
    scale_fill_brewer(palette = paleta_colores) +
    labs(
      title = titulo,
      x = etiqueta_x,
      y = etiqueta_y,
      caption = pie_de_pagina
    ) +
    theme_minimal() +
    theme(legend.position = "none")
}

Los argumentos de la función serán: 
a. El nombre del dataframe; y 
b. El rango de variables dicotómicas, en formato de índice (por ejemplo, c(1:3))

Pregúntame lo que sea necesario para darme tu mejor resultado

#### Resps


    ¿Qué formato tienen actualmente las variables dicotómicas?
        Son factores con niveles "Sí" y "No"


    ¿Cómo prefieres ordenar las variables en el gráfico?
        De mayor a menor porcentaje de "Sí"


    ¿Cómo debe manejarse el caso de valores no válidos en las variables seleccionadas?
        Simplemente excluir estos valores del cálculo?

    Personalización del gráfico:
        El estilo del gráfico debe ser igual al de la función graficar_porcentajes que compartí
        Quiero que el gráfico use una paleta de colores específica dentro de RColorBrewer que yo definiré (ponlo como argumento)

    Formato de la tabla:
        Usa kable() como en el ejemplo anterior 
        No, no necesito una columna adicional en la tabla con el total de observaciones

###C05
En R, tengo la función siguiente, la cual ejecuta una transformación box-cox de una serie de escalas originalmente no normalmente distribuidas, y luego interpreta los resultados: 

box_norma <- function(data) {
  # Verificar que las columnas sean numéricas
  data <- data[, sapply(data, is.numeric)]
  
  # Aplicar la transformación Box-Cox (manejo de valores <= 0)
  boxcox_transformada <- as.data.frame(lapply(data, function(column) {
    if (any(column <= 0)) {
      return(rep(NA, length(column))) # Box-Cox requiere valores positivos
    } else {
      # Crear un modelo lineal con un intercepto único para calcular Box-Cox
      df_column <- data.frame(y = column)  # Crear un dataframe explícito con la columna
      modelo <- lm(y ~ 1, data = df_column)  # Usar este dataframe en lm()
      
      # Calcular el lambda óptimo
      boxcox_res <- boxcox(modelo, lambda = seq(-2, 2, 0.1))
      lambda <- boxcox_res$x[which.max(boxcox_res$y)]
      
      # Aplicar la transformación Box-Cox
      if (lambda == 0) {
        return(log(column))
      } else {
        return((column^lambda - 1) / lambda)
      }
    }
  }))
  
  # Evaluar normalidad con Shapiro-Wilk y Lilliefors
  evaluar_normalidad <- function(variable) {
    if (length(variable[!is.na(variable)]) > 2) {
      shapiro_p <- tryCatch(shapiro.test(variable)$p.value, error = function(e) NA)
      ks_p <- tryCatch(nortest::lillie.test(variable)$p.value, error = function(e) NA)
    } else {
      shapiro_p <- ks_p <- NA
    }
    list(Shapiro_Wilk = shapiro_p, Lilliefors_KS = ks_p)
  }
  
  # Crear un dataframe para almacenar los resultados
  resultados <- data.frame(
    Escala = character(),
    Índice = character(),
    `p-valor` = character(),
    Interpretación = character(),
    stringsAsFactors = FALSE
  )
  
  interpretar_resultado <- function(p) {
    if (is.na(p)) return(c("NA", "Valor insuficiente"))
    nivel_significancia <- ifelse(p <= 0.001, "***",
                                  ifelse(p <= 0.01, "**",
                                         ifelse(p <= 0.05, "*", "NS")))
    interpretacion <- ifelse(p > 0.05, "Normalidad", "No Normalidad")
    c(sprintf("%.2f, %s", p, nivel_significancia), interpretacion)
  }
  
  # Iterar sobre cada columna
  for (var in colnames(boxcox_transformada)) {
    variable <- boxcox_transformada[[var]]
    resultados_test <- evaluar_normalidad(variable)
    
    # Agregar resultados de Shapiro-Wilk
    sw_resultado <- interpretar_resultado(resultados_test$Shapiro_Wilk)
    resultados <- rbind(resultados, data.frame(
      Escala = var,
      Índice = "S-W",
      `p-valor` = sw_resultado[1],
      Interpretación = sw_resultado[2],
      stringsAsFactors = FALSE
    ))
    
    # Agregar resultados de Lilliefors
    ks_resultado <- interpretar_resultado(resultados_test$Lilliefors_KS)
    resultados <- rbind(resultados, data.frame(
      Escala = var,
      Índice = "K-S, L",
      `p-valor` = ks_resultado[1],
      Interpretación = ks_resultado[2],
      stringsAsFactors = FALSE
    ))
  }
  
  # Mostrar resultados como tabla
  kable(resultados, format = "markdown", caption = "Transformación Box-Cox de las Escalas")
}


La función, sin embargo, me genera el siguiente error: 

Error in eval(mf, parent.frame()) : object 'df_column' not found


¿Puedes corregir este error?

#### C06

Tengo la siguiente función:

# Función ajustada
anal_di <- function(data, 
                    rango_indices, 
                    titulo_tabla = "Tabla de porcentajes de 'Sí' (1)", 
                    titulo_grafico = "Porcentajes de 'Sí' (1) por variable", 
                    color_contorno = "black", 
                    paleta_colores = "Dark2") {
  # Seleccionar las variables según el rango
  sub_data <- data[, rango_indices]
  
  # Verificar que las columnas sean dicotómicas
  if (!all(apply(sub_data, 2, function(x) all(x %in% c(0, 1))))) {
    stop("Todas las columnas seleccionadas deben ser dicotómicas (valores 0 y 1).")
  }
  
  # Calcular los porcentajes de "1" para cada variable
  tabla_porcentajes <- sub_data %>%
    summarise(across(everything(), ~ mean(.x) * 100)) %>%
    pivot_longer(cols = everything(), names_to = "Variable", values_to = "Porcentaje") %>%
    arrange(desc(Porcentaje)) %>%
    mutate(Porcentaje = round(Porcentaje, 2)) # Redondear a 2 decimales
  
  # Mostrar la tabla en formato Markdown usando kable
  tabla_md <- tabla_porcentajes %>%
    kable("markdown", col.names = c("Variable", "Porcentaje (%)"), caption = titulo_tabla) 
  # %>%
  #   kable_styling(full_width = FALSE, bootstrap_options = c("striped", "hover"))
  
  print(tabla_md)
  
  # Crear el gráfico de barras
  grafico <- ggplot(tabla_porcentajes, aes(x = reorder(Variable, -Porcentaje), y = Porcentaje, fill = Variable)) +
    geom_col(width = 0.75, color = color_contorno) +  # Agregar línea de contorno
    geom_text(aes(label = paste0(round(Porcentaje, 1), "%")), vjust = -0.5) +
    scale_fill_brewer(palette = paleta_colores) +  # Paleta definida por el usuario
    labs(
      title = titulo_grafico,
      x = "Variables",
      y = "Porcentaje",
      caption = "Fuente: Data Niveles 4-5"
    ) +
    theme_gray() +  # Cambiar tema a theme_gray
    theme(legend.position = "none")
  
  print(grafico)
}

                            
Necesito que la modifiques de tal modo que la data de las columnas sean factores en vez de valores numéricos. Los niveles de los factores serán siempre "0" y "1"



####C7

Pasa algo raro. Cuando ejecuto este chunk de código:

```{r 45_P1_Resumen_Binarios, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE, results='asis'}



library(dplyr)
library(ggplot2)
library(tidyr)
library(kableExtra)
library(RColorBrewer)

analizar_dicotomicas_numericas <- function(data, rango_indices, 
                                           titulo_tabla = "Tabla de porcentajes de 'Sí'", 
                                           titulo_grafico = "Porcentajes de 'Sí' por variable", 
                                           etiqueta_x = "Variables", 
                                           etiqueta_y = "Porcentaje", 
                                           pie_de_pagina = "Fuente: Datos internos. Elaboración propia", 
                                           paleta_colores = "Dark2", 
                                           color_contorno = "black") { # Nuevo argumento para el color del contorno
  # Seleccionar las columnas según el rango
  sub_data <- data[, rango_indices]
  
  # Verificar que las columnas sean numéricas con valores 0 y 1
  if (!all(sapply(sub_data, function(x) is.numeric(x) && all(x %in% c(0, 1))))) {
    stop("Todas las columnas seleccionadas deben ser numéricas con valores 0 y 1.")
  }
  
  # Calcular el porcentaje de "1" (Sí) para cada variable
  tabla_porcentajes <- sub_data %>%
    summarise(across(everything(), ~ mean(. == 1, na.rm = TRUE) * 100)) %>%
    pivot_longer(cols = everything(), names_to = "Variable", values_to = "Porcentaje") %>%
    arrange(desc(Porcentaje)) %>%
    mutate(Porcentaje = round(Porcentaje, 2)) # Redondear porcentajes
  
  # Mostrar la tabla con kable
  tabla_md <- tabla_porcentajes %>%
    kable("markdown", col.names = c("Variable", "Porcentaje (%)"), caption = titulo_tabla) 
  print(tabla_md)
  
  # Crear el gráfico de barras con contorno
  grafico <- ggplot(tabla_porcentajes, aes(x = reorder(Variable, -Porcentaje), y = Porcentaje, fill = Variable)) +
    geom_col(width = 0.75, color = color_contorno) + # Agregar el color del contorno
    geom_text(aes(label = paste0(Porcentaje, " %")), vjust = -0.5) +
    scale_fill_brewer(palette = paleta_colores) +
    labs(
      title = titulo_grafico,
      x = etiqueta_x,
      y = etiqueta_y,
      caption = pie_de_pagina
    ) +
    theme_minimal() +
    theme(legend.position = "none")
  
  print(grafico)
}




analizar_dicotomicas_numericas(data = INDI45,
        rango_indices = c(23:30),
        titulo_tabla = "Tabla de Frecuencia de Incidencias", 
        titulo_grafico = "Porcentajes de Incidencias", 
        etiqueta_x = "Incidencias", 
        etiqueta_y = "Porcentaje", 
        pie_de_pagina = "Fuente: Datos Niveles 4-5", 
        paleta_colores = "Greens",
         color_contorno = "black")

analizar_dicotomicas_numericas(data = INDI45,
        rango_indices = c(33:39),
        titulo_tabla = "Resumen de Tratamientos", 
        titulo_grafico = "Porcentajes de Tratamiento", 
        etiqueta_x = "Tratamiento", 
        etiqueta_y = "Porcentaje", 
        pie_de_pagina = "Fuente: Datos Niveles 4-5", 
        paleta_colores = "Blues",
         color_contorno = "black")



```

Funciona, pero cuando compilo el documento entero (que tiene otros chunks antes y después), se produce este error:

processing file: Info_Dinix_G45_P1_R.Rmd
                                                                                                                   
Error in `analizar_dicotomicas_numericas()`:
! Todas las columnas seleccionadas deben ser numéricas con valores 0 y 1.
Backtrace:
  1. rmarkdown::render("/cloud/project/Info_Dinix_G45_P1_R.Rmd", encoding = "UTF-8")
  2. knitr::knit(knit_input, knit_output, envir = envir, quiet = quiet)
  3. knitr:::process_file(text, output)
  6. knitr:::process_group(group)
  7. knitr:::call_block(x)
     ...
 14. base::withRestarts(...)
 15. base (local) withRestartList(expr, restarts)
 16. base (local) withOneRestart(withRestartList(expr, restarts[-nr]), restarts[[nr]])
 17. base (local) docall(restart$handler, restartArgs)
 19. evaluate (local) fun(base::quote(`<smplErrr>`))

Quitting from lines 1336-1425 [45_P1_Resumen_Binarios] (Info_Dinix_G45_P1_R.Rmd)
Execution halted

¿Cómo lo corrijo?

##C008

El siguiente código:

# Paquete necesario
library(knitr)

# Extraer las cargas factoriales estandarizadas del modelo
cargas <- inspect(afc30_01, what = "std")$lambda

# Convertir la matriz a un data.frame
cargas_df <- as.data.frame(cargas)
cargas_df <- cbind(Item = rownames(cargas_df), cargas_df)

# Identificar columnas con al menos una carga distinta de NA o 0
col_validas <- colSums(!is.na(cargas)) > 0  # Identifica factores con cargas
col_validas <- which(col_validas)           # Obtiene los índices de las columnas válidas

# Filtrar solo las columnas relevantes (ítems y factores con cargas válidas)
cargas_df <- cargas_df[, c(1, col_validas + 1)]  # "+1" porque "Item" está en la columna 1

# Renombrar columnas para mayor claridad
colnames(cargas_df)[-1] <- paste0("Factor", seq_len(ncol(cargas_df) - 1))

# Generar la tabla en formato Markdown con kable
kable(cargas_df, caption = "Cargas Factoriales Estandarizadas", digits = 3)


Genera una tabla con las cargas factoriales de los ítems, pero quisiera hacer un cambio estético. La tabla muestra por ejemplo:

|    |Item | Factor1| Factor2| Factor3| Factor4|
|:---|:----|-------:|-------:|-------:|-------:|
|C1  |C1   |   0.793|   0.000|   0.000|   0.000|
|C2  |C2   |   0.808|   0.000|   0.000|   0.000|

A la izquierda de la columna "Item" hay una primera columna que repite el dato que se muestra en la columna "Item". Necesito que edites el código para que la tabla *NO* muestre esa primera columna a la izquierda de la columna "Item".

## Tabla de ítems con cargas factoriales

Necesito que edites el código siguiente: 

# Extraer las cargas factoriales estandarizadas del modelo
cargas <- inspect(afc30_01, what = "std")$lambda

# Convertir la matriz a un data.frame
cargas_df <- as.data.frame(cargas)
cargas_df <- cbind(Item = rownames(cargas_df), cargas_df)

# Identificar columnas con al menos una carga distinta de NA o 0
col_validas <- colSums(!is.na(cargas)) > 0  # Identifica factores con cargas
col_validas <- which(col_validas)           # Obtiene los índices de las columnas válidas

# Filtrar solo las columnas relevantes (ítems y factores con cargas válidas)
cargas_df <- cargas_df[, c(1, col_validas + 1)]  # "+1" porque "Item" está en la columna 1

# Renombrar columnas para mayor claridad
colnames(cargas_df)[-1] <- paste0("Factor", seq_len(ncol(cargas_df) - 1))

# Generar la tabla en formato Markdown con kable sin la columna extra
kable(cargas_df, caption = "Cargas Factoriales Estandarizadas", digits = 3, row.names = FALSE)

A fin de que a la tabla resultante se añada una columna llamada "Contenido".

## C009

Ahora necesito que tomes el código siguiente:

# Extraer las cargas factoriales estandarizadas del modelo
cargas <- inspect(afc30_01, what = "std")$lambda

# Convertir la matriz a un data.frame
cargas_df <- as.data.frame(cargas)
cargas_df <- cbind(Item = rownames(cargas_df), cargas_df)

# Identificar columnas con al menos una carga distinta de NA o 0
col_validas <- colSums(!is.na(cargas)) > 0  # Identifica factores con cargas
col_validas <- which(col_validas)           # Obtiene los índices de las columnas válidas

# Filtrar solo las columnas relevantes (ítems y factores con cargas válidas)
cargas_df <- cargas_df[, c(1, col_validas + 1)]  # "+1" porque "Item" está en la columna 1

# Renombrar columnas para mayor claridad
colnames(cargas_df)[-1] <- paste0("Factor", seq_len(ncol(cargas_df) - 1))

# Generar la tabla en formato Markdown con kable sin la columna extra
kable(cargas_df, caption = "Cargas Factoriales Estandarizadas", digits = 3, row.names = FALSE)

Y lo modifiques de tal modo que la primera columna de la tabla resultante contenga la información contenida en la columna infovar$Contenido

Puedes hacerme todas las preguntas que consideres necesarias para darme tu mejor resultado.


1. ¿Los nombres de los ítems (en cargas_df$Item) son exactamente iguales a los de informacion_variables$Variable? No, pero están en exactamente el mismo orden. Solo necesitas reemplazar una columna por otra, no añadir sino reemplazar.
2. ¿El data frame informacion_variables está ya creado y contiene las columnas Variable y Descripcion? Sí, y ya es parte del environmente de modo que no habrá problema al ejecutar el código.
3. ¿Qué hacer si un ítem de cargas_df$Item no tiene descripción en informacion_variables? Eso no debería pasar, pero si llega a suceder, detén la ejecución y muestra una notificación explicando el error.
4. 


## Análisis de Ítems 

Tengo una escala diseñada para evaluar el desempeño escolar de alumnos de educación inicial. Necesito determinar qué tan bien discriminan los ítems que la componen en relación a los alumnos buenos y malos. ¿Qué tipo de análisis me recomiendas?


* Usando dplyr, y a partir de este ejemplo:

library(dplyr)

# Mover la columna `item_final` a la posición 3
datos <- datos %>%
  relocate(item_final, .before = col3)
  
Dame el código para reordenar múltiples columnas dentro de un solo comando



## Discri

Estoy tratando de calcular el índice de Discriminación mediante el siguiente código (el cual está adaptado de un ejemplo que me diste tú anteriormente):

# Discriminante Escala C
# Categorizamos a los participantes en grupo superior e inferior (27% each)
# Ordenar los datos según el puntaje total
DISCRIC <- DISCRI[order(DISCRI$CSUM, decreasing = TRUE), ]

# Definir el tamaño de los grupos (27% superior e inferior)
n <- nrow(DISCRIC) # Número total de estudiantes
grupo <- round(0.27 * n)

# Dividir los grupos
C_alto <- DISCRIC[1:grupo, ]    # 27% superior
C_bajo <- DISCRIC[(n-grupo+1):n, ] # 27% inferior
# Identificar las columnas de los ítems
C_items <- names(DISCRIC)[2:27] # Suponiendo que los ítems están en la 3ª columna en adelante
# Calcular el índice de discriminación para cada ítem
C_Discri <- sapply(C_items, function(item) {
  # Porcentaje de respuestas correctas en grupo alto
  p_alto <- mean(C_alto[[item]])
  
  # Porcentaje de respuestas correctas en grupo bajo
  p_bajo <- mean(C_bajo[[item]])
  
  # Índice de discriminación
  p_alto - p_bajo
})

El problema es que mostrar el índice de discriminación "C_Discri", el resultado en todos los casos es mayor a 1. ¿Qué está pasando y cómo lo corrijo?



El siguiente código:

# Discriminante Escala C
# Categorizamos a los participantes en grupo superior e inferior (27% each)
# Ordenar los datos según el puntaje total
DISCRIC <- DISCRI[order(DISCRI$CSUM, decreasing = TRUE), ]

# Definir el tamaño de los grupos (27% superior e inferior)
n <- nrow(DISCRIC) # Número total de estudiantes
grupo <- round(0.27 * n)

# Dividir los grupos
C_alto <- DISCRIC[1:grupo, ]    # 27% superior
C_bajo <- DISCRIC[(n-grupo+1):n, ] # 27% inferior
# Identificar las columnas de los ítems
C_items <- names(DISCRIC)[2:27] # Suponiendo que los ítems están en la 2ª columna en adelante
# Transformar las respuestas en dicotómicas (1 = alta, 0 = baja)
DISCRIC[C_items] <- lapply(DISCRIC[C_items], function(x) ifelse(x >= 4, 1, 0))

# Recalcula el índice de discriminación con las respuestas dicotómicas
C_Discri <- sapply(C_items, function(item) {
  # Proporción de respuestas altas en el grupo alto
  p_alto <- mean(C_alto[[item]])
  
  # Proporción de respuestas altas en el grupo bajo
  p_bajo <- mean(C_bajo[[item]])
  
  # Índice de discriminación
  p_alto - p_bajo
})

# Mostrar los índices de discriminación
print(C_Discri)

Me sigue arrojando exactamente los mismos resultados que con los valores originales de los ítems. ¿Como lo corrijo?